---
layout: post
#标题配置
title:  生成对抗网络GAN
#时间配置
date:   2017-08-22 11:04:00 +0800
#大类配置
categories: deep_learning
#小类配置
tag: 教程
---

* content
{:toc}
Photo-realistic high-resolution(高清晰度)

Y.Bengio：Generating high-resolution,photo-realistic images has been a long-standing goal in machine learning(NIPS 2016)

learning via generation(视觉，识别率)

ill-pose problem(病态问题)

Deep Generative Models(深度生成模型)

A Generative model is a model for randomly generationg observable data values,typically given some hidden parameters.it specifies a joint probability distribution over observation and label seqences.

why important？

1. Excellent test of our ability to use high- dimensional, complicated probability distributions （验证功能）
2. Simulate possible futures for planning or simulated ReinforceLearning（模拟功能）
3. Missing data（补全缺失数据）
4. Multi-modal outputs
5. Realistic generation tasks （有助于实现生成式任务）

 ![page17image2368](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/306F0A4600DB86301C540FE0F7317200.png)

Generative Adversarial Network(GAN)
-----------------------------------

No Markov chains needed

经常被认为产生的是最优的样本（并没有严格的证明）

A balance between **generative models** and **discriminative models**

 ![page19image2544](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/F118042D0B0AEB3960997532FE85F25B.png)

### Discriminator Strategy

\begin{align}
  &J^{(D)}=-\frac{1}{2}E_{x\thicksim p_{data}}logD(x)-\frac{1}{2}E_{z}log(1-D(G(Z)))\\
  &J^{(G)}=-J^{(D)}\\
  &D(x)=\frac{p_{data}(x)}{p_{data}(x)+p_{model}(x)}
\end{align}
G是一个生成图片的网络，他接收一个随机的噪声z，通过这个噪声生成图片，记为G(z)。
D是一个判别网络，x代表一张图片，输出D(x)代表x为真实图片的概率，0~1之间。
生成网络G的目标就是尽量生成真实的图片去欺骗判别网络D。D尽量把G生成的图片和真实的图片分别开来，G与D构成一个动态博弈的过程。
理想状态下，G产生以假乱真的图片G(z)，对D来说难以分辨G产生的图片是真是假，D(G(z))=0.5

GANs are generative models that 
 supervised learning(**the key approximation mechanism) **to approximate an intractable cost functions(监督学习同样也是很多其他问题所在的原因)

### Designing Generator

MPL / U-NET / CNN / Stacked CNN 

Auxiliary Classifier

 ![page24image6296](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/A3F3C9788FC69F9AE9CCD954DFC2C8DB.png) 

Auto-encoder

Multi task

### Training GANs

Train generator and discriminator **alternately**

Difficulties: 

1\. Convergence,

2\. Mode collapse, 

3\. Insufficient diversity, 

4\. Unpaired Translation

Goal: stability of training , resulting quality of GAN

1\. Convergence

Solution: 

* New Distribution Discrepancy Metrics,
* Feature matching,
* Balancing Mechanism

Distribution Discrepancy Metrics:

* Binary cross-entropy
* Pixel-wise loss
* Perceptual loss
* Maximum Mean Discrepancy

GANs 一般使用JS散度(Jensen-Shannon divergence)

\begin{align}
  &JS(P_r,P_g)=KL(P_r||P_m)+KL(P_g||P_m)\\
  &P_m=\frac{(P_r+P_g)}{2}
\end{align}

其中，KL表示KL散度，又称相对熵，描述两个概率分布的一种方法。KL散度不对称，KL(P||Q)$\ne$KL(Q||P)。
对于两个概率分布P和Q来说，其KL散度定义为

\begin{align}
  &D(P||Q)=\sum(P(i)\log{\frac{P(i)}{Q(i)}})\quad(离散情况)\\
  &D(P||Q)=\int P(x)\log{\frac{P(i)}{Q(i)}}d(x)\quad(连续情况)
\end{align}

KL散度是用来度量基于Q编码来编码来自P的样本平均所需的额外的比特数。
P表示数据的真实分布，Q表示数据的理论分布，模型分布，或者近似分布。
在信息论中，D(P||Q)表示用概率分布Q来拟合真实分布P的时候，产生的信息损耗。
$P_r$表示分类器的概率分布，$P_g$表示生成器的概率分布。
而现在，讲JS散度替换为Earth-Mover散度(EMD)，或者Wasserstein-1
\begin{align}
  W(P_r,P_g)=\inf_{\gamma \in \prod(P_r,P_g)}E_{(x,y)~\gamma}[\| x-y \|]
\end{align}
sup表示上界，inf表示下界。该函数的意思是寻找$P_r$和$P_g$使得x与y的差异最小。
EMD也是用来度量两个多维度的函数分布在多特征空间之间的差异，在已知其中一个特征之间的距离的情况下。
EMD lift this distance from individual features to full distribution.
two distribution, one can be seen as a mass of earth, the other can be seen as some holes.
the EMD measures the least amount of work needed to fill the hole with the earth.
EMD是一种EM算法，类似于最大流之类的。
假设从P工厂到Q仓库，P有$P_1,P_2,...,P_m$个工厂，Q有$Q_1,Q_2,...,Q_n$个工厂，P有重量为$w_{P_i}$的货物，Q有$w_{Q_i}$的容量，定义从$P_i$到$Q_j$,距离是$d_{ij}$,运送货物的重量为$f_{ij}$。
则工作量的总和为：
\begin{align}
W=\sum_{i=1}^{m}\sum_{j=1}^{n}d_{ij}f_{ij}\longrightarrow min
\end{align}

WGAN vs GAN

GAN 对JS散度进行优化，并且假设D有无限的模型化能力（D has infinite modeling capability）。

当真实数据和产生的数据可以完全分离的时候，JS散度的梯度变成了一个常数，使得梯度消失了。

WGAN使用的EMD则不会有这个问题。

 ![page31image2328](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/A35198F03FD10F0D122B4D74DF96F3E9.jpg)

Feature Matching
不是要将分类器的输出最大化，而是使得生成器产生的数据尽量贴近真实数据。
生成器的目标函数：
\begin{align}
  \| E_{x~p_{data}}f(x)-E_{z~p_z(z)}f(G(z))\|_2^2
\end{align}

Balancing Mechanism

保持分类器和生成器损失函数的平衡

discriminator have two competing goals：

auto-encode real images

discriminate real from generated image

\begin{align}
  \gamma =\frac{E[\ell(G(z))]}{E[\ell(x)]}
\end{align}

**至此，convergence部分结束，可以看到提出的解决方案全都是基于改变目标函数的基础上。**

2.Mode Collapse

‘’one of the main failure modes for GAN is for the generator to collapse to a parameter setting where it always emits the same point.”

Solution：

1\. minibatch discrimination

2\. repelling regularization

Minibatch Discrimination

key：allow the discriminator to look at multiple data examples in combination

在合理范围内，增大 Batch\_Size 有何好处？

* 内存利用率提高了，大矩阵乘法的并行化效率提高。
* 跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。
* 在一定范围内，一般来说 Batch\_Size 越大，其确定的下降方向越准，引起训练震荡越小。

盲目增大 Batch\_Size 有何坏处？

* 内存利用率提高了，但是内存容量可能撑不住了。
* 跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。
* Batch\_Size 增大到一定程度，其确定的下降方向已经基本不再变化。

Repelling Regularization

\begin{align}
  f_{PT}(S)=\frac{1}{N(N-1)}\sum_i\sum_{j\neq i}(\frac{S^T_iS_j}{\|S_i\|\|S_j\|})^2
\end{align}

The PT term is intended to decrease the magnitude of cosine similarity between pairwise sample representations, and thus making them as orthogonal as possible. 

**至此，Model Collapse部分结束**

3.Unpaired translation

 ![page39image5000](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/FEFE9940AA57F0F4BE3C6750C788176E.png) 

 ![page39image4832](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/FD195F1CE21E785F5F407D99F5ACE575.jpg)

Our goal is to learn a mapping G : X —\> Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. 

Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y -\> X and introduce a cycle consistency loss to push F(G(X)) = X

Towards Large-Pose Face Frontalization in the wild

Global Perception（全局感知）
-----------------------

 "the visual system is sensitive to global topological properties”(对于一幅图像，观察的优先级不同)

"global topological properties is a basic factor in perceptual organization” （这对于感受是很大的影响因素）

Global and Local Perception GAN

 ![page45image2400](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/87306A2DB95EAECF022499B4D1142D77.png) 

人在对一个正脸进行辨认时，先根据先验知识观察整体轮廓，然后转移到各个局部特征（眼睛，鼻子等）

* Fully Convolutional Neural Networks（FCNN）
* Residual Nets，CNN, Maxout Network
* Multi-task learning（Four local networks and one global networks）
* Five loss functions

Five loss functions：

Adversarial Loss

Identity Perserving Loss

Symmetrty（ill-posed problem）

Pixel Loss

Smooth Losses

The final synthesis loss function:
$L_{syn}=L_{pixel}+\lambda _1L_{sym}+\lambda _2L_{adv}+\lambda _3L_{ip}+\lambda _4L_{tv}$

对于很多文章都没能很好解决的ill-pose问题，Towards Large-Pose Face Frontalization GAN做出来的效果很好。

Huaibo Huang, Ran He, Zhenan Sun, Tieniu Tan. Wavelet-GAN: A Wavelet-based GAN for Multi-scale Face Super Resolution. 2017

Chaoyou Fu, Xiang Wu, Jing Dong, Ran He. Global Perception Feedback Convolutional Neural Networks. 2017.

Discriminative Model(判别模型)：Light Convolutional Neural Networks

Ordinal Measures(OM)

A Light CNN for Deep Face Representation with Noisy Labels.

lateral inhibition(侧抑制)

1. 侧抑制是指被激活的神经元减少对于相邻神经元的影响
2. 减弱了神经信号传播的扩散速度
3. 侧抑制主要的作用是对于脑中的某种视觉信号进行增强作用

visual inhibition（视觉抑制）

1. 侧抑制增强了视觉反馈中的对比度
2. 哺乳动物视网膜上经常发生
3. 侧抑制由水平的视觉细胞产生，可以让视觉信号更加concentrated and balanced。

A Light CNN for Deep Face Representation with Noisy Labels 2015.

Summary

Generative models：Autoencoder\\Generative Adversarial Network

Discriminative models：Convolutional neural networks

Temporal and continuous information：Recurrent neural networks



