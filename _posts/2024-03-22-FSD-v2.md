---
layout: post
title:  "FSD V2: Improving Fully Sparse 3D Object Detection with Virtual Voxels"
date:   2024-03-22 11:27:01 +0800
categories: deep_learning
tag: lidar_deep_learning
---


* content
{:toc}
FSD V2
![Outline of FSD](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/2024-03-22-113104.png)
### FSD V1
- backbone, use SST or FPP as the Point Feature Extraction, map the voxel features to point features.
![SST](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/2024-03-22-113305.png)
![FPP](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/2024-03-22-152148.png)
- MLP-based neck converts voxel features into point features.
- a lightweight pointwise MLP to perform point-wise classification and center voting.
- Connected Component Labeling to voted centerd to cluster points into instances.
- use sparse operators to extract instance festures for bounding box prediction.
- core operator: Sparse Instance Recognition(SIR)
  - instance point features to MLP -> instance-wise max-pooling get instacnce features -> broadcasted to each point within the instance and concat -> another MLP to reduce dimension. Do it iteratively.
  - The eventual instance features are obtained by max-pooling, and make the bbox predictions
- limitation
  - clustering is handcrafted

### FSD V2
- FSD v2 share the same structure with FSD v1 in backbone and point-wise classification and point-wise center voting.
- FSD V2 applies virtual voxelization instead of clustering -> use virtual voxel mixer to mix features of different virtual voxels. these 2 parts are equal to the SIR in FSD v1.

#### Virtual voxels
- FSD v2 use the virtual voxel for bounding box prediction, we do label assignment to the virtual voxel.
- For each foreground point, decode its voted object center from the predicted offset, then we apply voxelization to the union of voted centers and original real points.
  - Virtual voxels refer to those voxels containing at least onevoted center, while voxels containing only original real points are referred to as real voxels.
  - there are usually only a few virtual voxels for two reasons
    - The voted centers are often tightly closed to each other. Thus, if the network makes perfect center voting, the number of virtual voxels equals the number of objects
    - The size of virtual voxels is relatively larger than commonly used voxel sizes in the early stage

#### VVE
- virtual voxel encoder (VVE) to encode the virtual voxel features, inherit the SIR structure.
  - SIR treats the instance cluster as a point group and extracts the instance feature while VVE treats points in a voxel as a group to extract the voxel feature
  - before voxel encoding, we need to first generate features for each voted center
  - voted centers copy the features from their corresponding real points, which are encoded by the sparse backbone; we append the voting offsets to voted centers as additional features to distinguish them from the real points; For real points, we pad zeros as pseudo offsets to maintain consistent feature dimensions
  - the virtual voxel feature encoder encodes voxel features by the SIR module, which can be viewed as a couple of PointNet layers; the virtual voxel encoder actually encodes features of both virtual voxels and real voxels.

#### VVM
- we attain different types of features,virtual voxel features, real voxel features, and multi-scale features from the SparseUNet backbone.
- if the voting res is not good, an whole object would be split to some parts and the virtual voxel features can not iteracte with each other, so we need to enable feature interaction between virtual voxels
![voting not good enough](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/2024-03-22-174256.png)
- we also need to do further enhancement by mixing virtual voxel features with real voxel features; virtual voxels are generated from predicted foreground points, which may occasionally miss certain foreground points, leading to potential information loss, which is FSD v1 not include.
- we propose to aggregate multi-scale features from backbone via VVM.
  - multi-scale real voxel features from different layers of the SparseUNet decoder
  - virtual/real voxel features
- but we cannot directly employ addition or feature concatenation along channel dimension since the features are spatially sparse.
- sparse location convert and aggregate.
  - calculate the feature location of different stride. assue the gray features is the final resolution features, we calculate the location of red faetures with $I_s^1 = I_s * s + \lfloor \frac{s}{2} \rfloor$; and use dynamic pooling to unique the duplicated coordinates; The features of duplicated voxels are averaged into a single feature just like the final image.

![mixing multi-scale feature](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/2024-03-22-171410.png)
- after mix with to enable feature interaction between virtual voxels, we use a straightforward lightweight SparseUNet as VVM model.
- The SparseUNet is highly efficient, featuring only three stages with different strides, each consisting of two sparse convolution layers, the output voxels have the same spatial distribution and resolution as the input voxels


#### Virtual Voxel Assignment
- we adopt a more straightforward strategy calledvoxel-in-box assignment, where all the virtual voxels falling within a bounding box are assigned as positive. 训练的时候所有在真值框里的都算作正样本。同时2D不能这么做的原因是2D框里并不只包含目标像素，还会有一些 其他的像素。
- default virtual voxel size of 0.5m × 0.5m × 0.5m; the bollard and traffic coneclasses are only around 0.1m × 0.1m in Bird Eye's View.it is hard to judge if such a large voxel is inside the tiny object
- the position of a virtual voxel is defined as the weighted centroid of its containing points
- Formally, the weighted centroid is defined as below, This weighting strategy indicates that the position of a voxel is primarily determined by its containing foreground points, while also slightly influenced by background points.
$$
    \bar{x} = \frac{\sum_{i=0}^{N-1} I(X_i)X_i}{\sum_{i=0}^{N-1} I(X_i)}
$$
$$
I(x) = 
    \begin{cases}
        1 & \text{if } X \in F \\
        \alpha & \text{if } X \notin F
    \end{cases}
$$
![Assign](https://github.com/Colorfu1/Colorful.io/blob/master/_posts/resources/2024-03-22-175923.png)

#### Virtual Voxel Head
- Virtual voxel features from VVM are further passed into a couple of MLPs for final box prediction
- We adopt Focal Loss with default hyperparameters for classification. We use L1Loss in the regression branch. The regression target is parameterized as (∆x, ∆y, ∆z, log l, log w, log h, sin θ, cos θ). ∆x is the x-offset from the bounding box gravity center to geometric center of the assigned virtual voxel. l/w/h is the three box dimensions, and θ is box heading.