---
layout: post
title:  "Latte: Latent Diffusion Transformer for Video Generation"
date:   2024-03-24 16:52:01 +0800
categories: deep_learning
tag: stable_diffusion
---


* content
{:toc}
### Latte
- We propose a novel Latent Diffusion Transformer, namely Latte, for video generation. Latte first extracts spatio-temporal tokens from input videos and then adopts a series of Transformer blocks to model video distribution in the latent space.
  - Latte adopts a video Transformer as the backbone, four model variants are introduced to efficiently capture spatiotemporal distribution in videos.
  - we comprehensively explore video clip patch embedding, model 
  variants, timestep-class information injection, temporal positional embedding, and learning strategies
  - Latte shows comparable results when applied to the text-to-video generation task.
### Methods
- $x \in p_{data}(x)$ -> $z = \mathscr{E}(x), \epsilon is the encoder $ ->  diffusion: $z_t = \sqrt{\bar{\alpha _t}} z + \sqrt{1- \bar{\alpha_t}} \epsilon $ -> predict $z_{t-1} : p_\theta(z_{t-1} | z_t) = \mathscr{N}(\mu_\theta(z_t), \Sigma_{\theta}(z_t))$
- extend the LDMs for video generation
  - encoder $\mathscr{E}$ encode each video frame into latent space
  - The diffusion process model the latent spatial and temporal information.
![pipeline]()
#### the model variants of Latte
- Variant 1
  - spatial Transformer blocks and temporal Transformer blocks. The former capturing spatial information exclusively among tokens sharing the same temporal index, the latter captures temporal information across temporal dimensions in an "interleaved fusion" manner
  - if we have latent space data $V_L \in R^{F*H*W*C}$, translate it to a sequence of token $\hat{z} \in R^{n_f*n_h*n_w*d}$, add posotional embeding p to z, get $z = \hat{z} + p$ as the input.
  - reahpe $z$ to $z_s = R^{n_f*t*d} where t = n_h * n_w$ as the input of spatial transformer. reshape $z_s$ to $z_t \in R^{t*n_f*d}$ as the input to temporal transformer block
- Variant 2
  - In contrast to the temporal "interleaved fusion" design in Variant 1, this variant utilizes the "late fusion" approach to combine spatio-temporal information.(3 spatial first then 3 temporal)
- Variant 3
  - Variant 3 focuses on decomposing the multi-head attention in the Transformer block; As a result, each Transformer block captures both spatial and temporal information
- Variant 4
  - decompose the multi-head attention (MHA) into two components in this variant, use different components to handle tokens separately in spatial and temporal dimensions.
- After the Transformer backbone, derive both predicted noise and predicted covariance. The shape of the two outputs is the same as that of the input $V_L \in R^{F*H*W*C}$ by standard linear decoder.
#### Latent video clip patch embeding

