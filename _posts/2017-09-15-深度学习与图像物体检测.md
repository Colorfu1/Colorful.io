layout: post
#标题配置
title:  深度学习与图像物体检测
#时间配置
date:   2017-09-15 16:04:00 +0800
#大类配置
categories: deeplearning
#小类配置
tag: 教程
---

* content
{:toc}
Classification: what

Detection: what and where

Part1: 经典物体检测算法
===============

Viola-Jones face detector(Robust Real-Time Face Detection.IJCV 2004)

自然图像中的挑战： view point variation, illumination, occlusion, scale, deformation, background clutter

经典物体检测算法框架
----------

* 在图像中通过滑动窗口（多尺度）生成候选
* 对每一个人候选，用分类器判别是否待检测物体：人工特征提取，分类器学习
* 对候选对象根据分类器score进行筛选

Step 1:两类分类器训练

 Given the representation, train a binary classifer

Step 2:滑窗生成候选

Scans the detector at multiple locations and scales

Viola-Jones detection approach
------------------------------

Implemented in OpenCV

### Components

* Features: Haar-features, Integral image
* Learning: Boosting algorithm
* Cascade method

**1.Features**

**Haar-features**

The difference between pixels’ sum of the white and black areas

 ![page16image3136](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/8A47E772A3AE69B83E3B6F6CEC083E6B.png)

Too many features, but not all of them are useful !

so we have speed-up strategy: integral image, to fast calculation of haar-features

**integral image**

sum of pixel values in the blue area

![abcd.tiff](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/A2D79F879B77EA79373A45B92640960F.jpg)

上图中，计算四个颜色的图像的像素值之和，若使用integral image的方法，则只需要计算d-c-b+a的值即可。如不使用integral，则需要将四个颜色区域的所有像素值相加（很可能是成千上万的规模），计算量极大。

![page16image3136](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/8A47E772A3AE69B83E3B6F6CEC083E6B.png)

类似的，对于A和B只需要计算六个点，六次加减法运算；对于C只需要计算八个点；对于D只需要计算九个点。

但是，对于选出来的Haar特征，只有很小的一部分可以用于分类，Viola-Jones detection approach 选择使用Adaboost方法来进行特征的选择。

**2\. Adaboost**

combining several weak classifiers to generate a strong classifer.

Each training sample may have different importance !

Focuses more on previous misclassified samples:

* Misclassified-\>increase weights
* Correctly classified -\>decrease their weights

 ![page37image504](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/F590B93444D045DF5FAEAFAD6FD4C2EA.png)

**3\. Cascade method**

each image has a lot of possible windows to search.so we use cascade method

 ![page40image3432](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/DEC313047C8299DB5D7D9ED9DD80E64B.png) 

most windows contain no face, rejects negative windows in an early stage.

经典物体检测算法

• 逐像素滑动窗口

• 多尺度滑动窗口

• 对每一个窗口输入分类器判断是否包含待检物体 

• 期待在某一个尺度下，某一个窗口包含物体

• 人工特征设计，简单高效计算

• Adaboost特征选择，分类器学习

Part 2:卷积神经网络
===============

* 特征学习
* 滑动窗口
* 多尺度

1. Traditional pattern recognition : feature extractoe-\>trainable classifier
2. Mainstream modern pattern recogniton : feature extractor-\>mid-level features-\>trainable classifier
3. Deep learning : low-level features-\>mid-level features-\>high-level features-\>trainable classifier

 ![page46image1376](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/AD8195F5162A390319DC4B2276C99F34.png)

convolution，pooling已经总结过....

卷积神经网络：

* 最佳图像建模工具
* 基本原理
* 卷积操作
* 降采样pooling
* 构建基本单元

**1.最佳图像建模工具**

针对图像处理设计的网络结构

传统神经网络将图像拉成一个向量，参数太多，计算效率低，忽略了图像的二维结构，忽略了空间上下文，不够鲁棒
**2.基本原理**

利用图像局部结构进行卷积操作
**3.卷积操作**

权值共享，局部视野感受

逐像素滑动

对图像进行滤波处理

若原始图像为200\*200，用5\*5的滤波器进行卷积操作，得到的滤波图像为196\*196的图像。

若采用全连接，权重数为:200\*200\*196\*196(将原图像的200\*200个像素映射到滤波图像的一个像素上，每次映射所使用的参数都不同，这个过程要196\*196次)，若采用局部连接，权重数为：25\*196\*196（用5\*5的滤波器，每25个像素映射到滤波图像的一个像素上，每个滤波器的参数都不同，共要196\*196次），采用局部连接+权值共享：一共25个权重（使用一个滤波器，滑动经过整个原始图像）。
**4.降采样pooling**

增强网络的鲁棒性，抗变性，位移干扰
**5.构建基本单元**

卷积层和降采样层交替进行，构建深度网络

基本构建单元：卷积+降采样

 ![page54image1232](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/87C310F3FCE25A8B10F3914AAA8A4A8A.png)

Part 3:基于CNN的物体检测（RCNN,Fast RCNN,Faster RCNN）
============================================================

INTRODUCTION TO IMAGENET:

1000 classes, each image has 1 class, at least one bounding box, 800 training images per class, algorithm produces 5(class, box) guesses, example is correct if at least one guess is correct class AND bounding box at least 0.5 interdection over union(LOU)

### Idea 1: localization as regression

Step 1: Train (or download) a classification model (AlexNet, VGG, GoogLeNet) 

![step 1.tiff](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/FDAEFF28891B35C0324BD0B2352673D9.jpg)

Step 2: Attach new fully-connected “regression head” to the network 

![step 2.tiff](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/62A050CFC234FDF5394E888E7B40CA8D.jpg)

Step 3: Train the regression head only with SGD and L2 loss 

![step 3.tiff](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/D056B963E0160D50FE7461F4629F624A.jpg)

Step 4: At test time use both heads 

![step 4.tiff](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/F8D55ABC52F7C38830144DD3B5CB8183.jpg)

![step 5.tiff](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/2ECBD53D38ADEEACC789CBB8D52A13EB.jpg)

![step 6.tiff](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/AE6143D328E4F4CD5AC27869485EFEFF.jpg)

![step 7.tiff](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/D057D7E64A6DE964C54681A7EE0E8787.jpg)

**(DeepPose: Human Pose Estimate via Deep Neural Networks, CVPR,2014)**

* very simple
* Pre-trained classification CNN
* Fine-tune on localization task
* Think if you can use this for projects

### Idea 2: Sliding Window

* 滑窗:Run classification + regression network at multiple locations on a high-resolution image
* 效率:Convert fully-connected layers into convolutional layers for efficient computation
* 融合:Combine classifier and regressor predictions across all scales for final prediction

Winner of ILSVRC 2013 localization challenge: " Integrated Recognition, Localization and Detection Convolutional Networks”,ICLR 2014

Overfeat:

![sliding.tiff](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/7128A401535332EB6094FFF81E3F826C.jpg)

图像大小是257\*257的，选用的滑窗是221\*221的（3\*221\*221表示其是RGB三通道的图像），使用滑窗划过整个图像，需要检测4次，得到四个滑窗内cat出现的概率分别为0.5，0.75，0.6，0.8。

实际使用中，通常在很多位置上使用不同尺度的滑窗。

Efficient Sliding Window: Overfeat

![Overfeat.tiff](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/F331158BE4433CE31EC2EE8B3054A6CC.jpg)

全连接层是个什么东西？

作者：魏秀参
链接：https://www.zhihu.com/question/41037974/answer/150522307
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

全连接层到底什么用？我来谈三点。

* 全连接层（fully connected layers，FC）在整个卷积神经网络中起到“分类器”的作用。如果说卷积层、池化层和激活函数层等操作是将原始数据映射到隐层特征空间的话，全连接层则起到将学到的“分布式特征表示”映射到样本标记空间的作用。在实际使用中，全连接层可由卷积操作实现：对前层是全连接的全连接层可以转化为卷积核为1x1的卷积；而前层是卷积层的全连接层可以转化为卷积核为hxw的全局卷积，h和w分别为前层卷积结果的高和宽（***注1***）。
* 目前由于全连接层参数冗余（仅全连接层参数就可占整个网络参数80%左右），近期一些性能优异的网络模型如ResNet和GoogLeNet等均用全局平均池化（global average pooling，GAP）取代FC来融合学到的深度特征，最后仍用softmax等损失函数作为网络目标函数来指导学习过程。需要指出的是，用GAP替代FC的网络通常有较好的预测性能。具体案例可参见我们在ECCV'16（视频）表象性格分析竞赛中获得冠军的做法：[「冠军之道」Apparent Personality Analysis竞赛经验分享 - 知乎专栏](https://zhuanlan.zhihu.com/p/23176872) ，project：[Deep Bimodal Regression for Apparent Personality Analysis**](https://link.zhihu.com/?target=http%253A//210.28.132.67/weixs/project/APA/APA.html)
* 在FC越来越不被看好的当下，我们近期的研究发现，FC可在模型表示能力迁移过程中充当“防火墙”的作用。具体来讲，假设在ImageNet上预训练得到的模型为![\\mathcal{M}](https://www.zhihu.com/equation?tex=%5Cmathcal%7BM%7D) ，则ImageNet可视为源域（迁移学习中的source domain）。微调（fine tuning）是深度学习领域最常用的迁移学习技术。针对微调，若目标域（target domain）中的图像与源域中图像差异巨大（如相比ImageNet，目标域图像不是物体为中心的图像，而是风景照，见下图），不含FC的网络微调后的结果要差于含FC的网络。因此FC可视作模型表示能力的“防火墙”，特别是在源域与目标域差异较大的情况下，FC可保持较大的模型capacity从而保证模型表示能力的迁移。（冗余的参数并不一无是处。）
* 

以VGG-16为例，对224x224x3的输入，最后一层卷积可得输出为7x7x512，如后层是一层含4096个神经元的FC，则可用卷积核为7x7x512x4096的全局卷积来实现这一全连接运算过程，其中该卷积核参数如下：

“filter size = 7, padding = 0, stride = 1, D\_in = 512, D\_out = 4096”

经过此卷积操作后可得输出为1x1x4096。

如需再次叠加一个2048的FC，则可设定参数为“filter size = 1, padding = 0, stride = 1, D\_in = 4096, D\_out = 2048”的卷积层操作。

 ![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/00F534BCEE243866BC8B80D1D5407827)

对于一个训练好的CNN来说，CNN的结构（如CNN的层数、每一层feature map的个数，卷积层的kernel size等等）是固定的，但是，每一层的feature map的大小是可以改变的。当测试样本和训练样本大小相同时，CNN最后一层的每一个节点分别输出一个0~1的实数，代表测试样本属于某一类的概率；当测试样本比训练样本大时，CNN最后一层每一个节点的输出为一个矩阵，矩阵中的每一个元素表示对应的图像块属于某一类的概率，其结果相当于通过滑窗从图像中进行采样，然后分别对采样到的图像块进行操作。

如上图所示，当输入图像为16x16时，output层的每一个节点输出为一个2x2的矩阵，相当于在原图像上的四个角分别采样了一个14x14的图像块，然后分别对每一个图像块进行处理。

将上面所说的，使用一个较小的滑窗对整个图像进行卷积，将其变成卷积神经网络的形式就是这样的。

![page81image2280](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/425B8A05F669B0729A894F5F8FE66BA3.jpg) 

AlexNet: Localization method not published 

Overfeat: Multiscale convolutional regression with box merging

VGG: Same as Overfeat, but fewer scales and locations; simpler method, gains all due to deeper features 

ResNet: Different localization method (RPN) and much deeper features 

![page86image3912](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/A4D46088575CE4197ECD14CF86C3CAF1.jpg) 

遇到这样的多物体检测的时候

Problem: Need to test many positions and scales,
 and use a computationally demanding classifier (CNN)

Solution: Only look at a tiny subset of possible positions 

### Region proposal

* ● Find “blobby” image regions that are likely to contain objects

  ● “Class-agnostic” object detector

  ● Look for “blob-like” regions

### R-CNN

转述自：<http://blog.csdn.net/hjimce/article/details/50187029>

**作者：**hjimce

**一、相关理论**

 本篇博文主要讲解2014年CVPR上的经典paper：《Rich feature hierarchies for Accurate Object Detection and Segmentation》，这篇文章的算法思想又被称之为：R-CNN（Regions with Convolutional Neural Network Features），是物体检测领域曾经获得state-of-art精度的经典文献。

 这篇paper的思想，改变了物体检测的总思路，现在好多文献关于深度学习的物体检测的算法，基本上都是继承了这个思想，比如：《Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition》，所以学习经典算法，有助于我们以后搞物体检测的其它paper。

 之前刚开始接触物体检测算法的时候，老是分不清deep learning中，物体检测和图片分类算法上的区别，弄得我头好晕，终于在这篇paper上，看到了解释。物体检测和图片分类的区别：图片分类不需要定位，而物体检测需要定位出物体的位置，也就是相当于把物体的bbox检测出来，还有一点物体检测是要把所有图片中的物体都识别定位出来。

**二、基础知识**

**1、有监督预训练与无监督预训练**

(1)无监督预训练(Unsupervised pre-training)

无监督预训练这个名词我们比较熟悉，栈式自编码、DBM采用的都是采用无监督预训练。因为预训练阶段的样本不需要人工标注数据，所以就叫做无监督预训练。

(2)有监督预训练(Supervised pre-training)

所谓的有监督预训练，我们也可以把它称之为迁移学习。比如你已经有一大堆标注好的人脸年龄分类的图片数据，训练了一个CNN，用于人脸的年龄识别。然后当你遇到新的项目任务是：人脸性别识别，那么这个时候你可以利用已经训练好的年龄识别CNN模型，去掉最后一层，然后其它的网络层参数就直接复制过来，继续进行训练。这就是所谓的迁移学习，说的简单一点就是把一个任务训练好的参数，拿到另外一个任务，作为神经网络的初始参数值,这样相比于你直接采用随机初始化的方法，精度可以有很大的提高。

图片分类标注好的训练数据非常多，但是物体检测的标注数据却很少，如何用少量的标注数据，训练高质量的模型，这就是文献最大的特点，这篇paper采用了迁移学习的思想。文献就先用了ILSVRC2012这个训练数据库（这是一个图片分类训练数据库），先进行网络的图片分类训练。这个数据库有大量的标注数据，共包含了1000种类别物体，因此预训练阶段cnn模型的输出是1000个神经元，或者我们也直接可以采用Alexnet训练好的模型参数。

**2、IOU的定义**

因为没有搞过物体检测不懂IOU这个概念，所以就简单介绍一下。物体检测需要定位出物体的bounding box，就像下面的图片一样，我们不仅要定位出车辆的bounding box 我们还要识别出bounding box 里面的物体就是车辆。对于bounding box的定位精度，有一个很重要的概念，因为我们算法不可能百分百跟人工标注的数据完全匹配，因此就存在一个定位精度评价公式：IOU。

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/C54942E61338945FC2DF9E9D5A58BCB4.png)

IOU定义了两个bounding box的重叠度，如下图所示：

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/82F1B0CAF132DE30178CB62EF5BC7EB9.png)

矩形框A、B的一个重合度IOU计算公式为：

IOU=(A∩B)/(A∪B)

就是矩形框A、B的重叠面积占A、B并集的面积比例:

IOU=SI/(SA+SB-SI)

**3、非极大值抑制**

因为一会儿讲RCNN算法，会从一张图片中找出n多个可能是物体的矩形框，然后为每个矩形框为做类别分类概率：

![](http://img.blog.csdn.net/20160314193846683)

就像上面的图片一样，定位一个车辆，最后算法就找出了一堆的方框，我们需要判别哪些矩形框是没用的。非极大值抑制：先假设有6个矩形框，根据分类器类别分类概率做排序，从小到大分别属于车辆的概率分别为A、B、C、D、E、F。

(1)从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值;

(2)假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。

(3)从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。

就这样一直重复，找到所有被保留下来的矩形框。

**4、VOC物体检测任务**

这个就相当于一个竞赛，里面包含了20个物体类别：<http://host.robots.ox.ac.uk/pascal/VOC/voc2012/examples/index.html> 还有一个背景，总共就相当于21个类别，因此一会设计fine-tuning CNN的时候，我们softmax分类输出层为21个神经元。

**三、**算法总体思路 ****

 开始讲解paper前，我们需要先把握总体思路，才容易理解paper的算法。

 图片分类与物体检测不同，物体检测需要定位出物体的位置，这种就相当于回归问题，求解一个包含物体的方框。而图片分类其实是逻辑回归。这种方法对于单物体检测还不错，但是对于多物体检测就……

 因此paper采用的方法是：首先输入一张图片，我们先定位出2000个物体候选框，然后采用CNN提取每个候选框中图片的特征向量，特征向量的维度为4096维，接着采用svm算法对各个候选框中的物体进行分类识别。也就是总个过程分为三个程序：a、找出候选框；b、利用CNN提取特征向量；c、利用SVM进行特征向量分类。具体的流程如下图片所示：

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/DE2407B616B82AB845D2EA71966B5A29.png)

后面我们将根据这三个过程，进行每个步骤的详细讲解。

**四、****候选框搜索阶段**

**1、实现方式**

当我们输入一张图片时，我们要搜索出所有可能是物体的区域，这个采用的方法是传统文献的算法：《search for object recognition》，通过这个算法我们搜索出2000个候选框。然后从上面的总流程图中可以看到，搜出的候选框是矩形的，而且是大小各不相同。然而CNN对输入图片的大小是有固定的，如果把搜索到的矩形选框不做处理，就扔进CNN中，肯定不行。因此对于每个输入的候选框都需要缩放到固定的大小。下面我们讲解要怎么进行缩放处理，为了简单起见我们假设下一阶段CNN所需要的输入图片大小是个正方形图片227\*227。因为我们经过selective search 得到的是矩形框，paper试验了两种不同的处理方法：

**(1)各向异性缩放**

这种方法很简单，就是不管图片的长宽比例，管它是否扭曲，进行缩放就是了，全部缩放到CNN输入的大小227\*227，如下图(D)所示；

**(2)各向同性缩放**

因为图片扭曲后，估计会对后续CNN的训练精度有影响，于是作者也测试了“各向同性缩放”方案。这个有两种办法

A、直接在原始图片中，把bounding box的边界进行扩展延伸成正方形，然后再进行裁剪；如果已经延伸到了原始图片的外边界，那么就用bounding box中的颜色均值填充；如下图(B)所示;

B、先把bounding box图片裁剪出来，然后用固定的背景颜色填充成正方形图片(背景颜色也是采用bounding box的像素颜色均值),如下图(C)所示;

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/0D2612CB982A6C703E914137C1DD07D3.png)

对于上面的异性、同性缩放，文献还有个padding处理（用于填充图片以适应大小），上面的示意图中第1、3行就是结合了padding=0,第2、4行结果图采用padding=16的结果。经过最后的试验，作者发现采用各向异性缩放、padding=16的精度最高，具体不再啰嗦。

OK，上面处理完后，可以得到指定大小的图片，因为我们后面还要继续用这2000个候选框图片，继续训练CNN、SVM。然而人工标注的数据一张图片中就只标注了正确的bounding box，我们搜索出来的2000个矩形框也不可能会出现一个与人工标注完全匹配的候选框。因此我们需要用IOU为2000个bounding box打标签，以便下一步CNN训练使用。在CNN阶段，如果用selective search挑选出来的候选框与物体的人工标注矩形框的重叠区域IoU大于0.5，那么我们就把这个候选框标注成物体类别，否则我们就把它当做背景类别。SVM阶段的正负样本标签问题，等到了svm讲解阶段我再具体讲解。

**五、CNN特征提取阶段**

****

**1、算法实现**

**a、网络结构设计阶段**

网络架构我们有两个可选方案：第一选择经典的Alexnet；第二选择VGG16。经过测试Alexnet精度为58.5%，VGG16精度为66%。VGG这个模型的特点是选择比较小的卷积核、选择较小的跨步，这个网络的精度高，不过计算量是Alexnet的7倍。后面为了简单起见，我们就直接选用Alexnet，并进行讲解；Alexnet特征提取部分包含了5个卷积层、2个全连接层，在Alexnet中p5层神经元个数为9216、 f6、f7的神经元个数都是4096，通过这个网络训练完毕后，最后提取特征，**每个输入候选框图片都**能得到一个4096维的特征向量。

**b、网络有监督预训练阶段**

参数初始化部分：物体检测的一个难点在于，物体标签训练数据少，如果要直接采用随机初始化CNN参数的方法，那么目前的训练数据量是远远不够的。这种情况下，最好的是采用某些方法，把参数初始化了，然后在进行有监督的参数微调，这边文献采用的是有监督的预训练。所以paper在设计网络结构的时候，是直接用Alexnet的网络，然后连参数也是直接采用它的参数，作为初始的参数值，然后再fine-tuning训练。

网络优化求解：采用随机梯度下降法，学习速率大小为0.001；

**C、fine-tuning阶段**

我们接着采用selective search 搜索出来的候选框，然后处理到指定大小图片，继续对上面预训练的cnn模型进行fine-tuning训练。假设要检测的物体类别有N类，那么我们就需要把上面预训练阶段的CNN模型的最后一层给替换掉，替换成N+1个输出的神经元(加1，表示还有一个背景)，然后这一层直接采用参数随机初始化的方法，其它网络层的参数不变；接着就可以开始继续SGD训练了。开始的时候，SGD学习率选择0.001，在每次训练的时候，我们batch size大小选择128，其中32个事正样本、96个事负样本（正负样本的定义前面已经提过，不再解释）。

**位置精修**

目标检测问题的衡量标准是重叠面积：许多看似准确的检测结果，往往因为候选框不够准确，重叠面积很小。故需要一个位置精修步骤。 
回归器 
对每一类目标，使用一个线性脊回归器进行精修。正则项λ=10000。 
输入为深度网络pool5层的4096维特征，输出为xy方向的缩放和平移。 
训练样本 
判定为本类的候选框中，和真值重叠面积大于0.6的候选框。

**2、问题解答**

OK，看完上面的CNN过程后，我们会有一些细节方面的疑问。首先，反正CNN都是用于提取特征，那么我直接用Alexnet做特征提取，省去fine-tuning阶段可以吗？这个是可以的，你可以不需重新训练CNN，直接采用Alexnet模型，提取出p5、或者f6、f7的特征，作为特征向量，然后进行训练svm，只不过这样精度会比较低。那么问题又来了，没有fine-tuning的时候，要选择哪一层的特征作为cnn提取到的特征呢？我们有可以选择p5、f6、f7，这三层的神经元个数分别是9216、4096、4096。从p5到p6这层的参数个数是：4096\*9216 ，从f6到f7的参数是4096\*4096。那么具体是选择p5、还是f6，又或者是f7呢？

文献paper给我们证明了一个理论，如果你不进行fine-tuning，也就是你直接把Alexnet模型当做万金油使用，类似于HOG、SIFT一样做特征提取，不针对特定的任务。然后把提取的特征用于分类，结果发现p5的精度竟然跟f6、f7差不多，而且f6提取到的特征还比f7的精度略高；如果你进行fine-tuning了，那么f7、f6的提取到的特征最会训练的svm分类器的精度就会飙涨。

据此我们明白了一个道理，如果不针对特定任务进行fine-tuning，而是把CNN当做特征提取器，卷积层所学到的特征其实就是基础的共享特征提取层，就类似于SIFT算法一样，可以用于提取各种图片的特征，而f6、f7所学习到的特征是用于针对特定任务的特征。打个比方：对于人脸性别识别来说，一个CNN模型前面的卷积层所学习到的特征就类似于学习人脸共性特征，然后全连接层所学习的特征就是针对性别分类的特征了。

还有另外一个疑问：CNN训练的时候，本来就是对bounding box的物体进行识别分类训练，是一个端到端的任务，在训练的时候最后一层softmax就是分类层，那么为什么作者闲着没事干要先用CNN做特征提取（提取fc7层数据），然后再把提取的特征用于训练svm分类器？这个是因为svm训练和cnn训练过程的正负样本定义方式各有不同，导致最后采用CNN softmax输出比采用svm精度还低。

事情是这样的，cnn在训练的时候，对训练数据做了比较宽松的标注，比如一个bounding box可能只包含物体的一部分，那么我也把它标注为正样本，用于训练cnn；采用这个方法的主要原因在于因为CNN容易过拟合，所以需要大量的训练数据，所以在CNN训练阶段我们是对Bounding box的位置限制条件限制的比较松(IOU只要大于0.5都被标注为正样本了)；

然而svm训练的时候，因为svm适用于少样本训练，所以对于训练样本数据的IOU要求比较严格，我们只有当bounding box把整个物体都包含进去了，我们才把它标注为物体类别，然后训练svm，具体请看下文。

**六、SVM训练、测试阶段**

这是一个二分类问题，我么假设我们要检测车辆。我们知道只有当bounding box把整量车都包含在内，那才叫正样本；如果bounding box 没有包含到车辆，那么我们就可以把它当做负样本。但问题是当我们的检测窗口只有部分包好物体，那该怎么定义正负样本呢？作者测试了IOU阈值各种方案数值0,0.1,0.2,0.3,0.4,0.5。最后我们通过训练发现，如果选择IOU阈值为0.3效果最好（选择为0精度下降了4个百分点，选择0.5精度下降了5个百分点）,即当重叠度小于0.3的时候，我们就把它标注为负样本。一旦CNN f7层特征被提取出来，那么我们将为每个物体累训练一个svm分类器。当我们用CNN提取2000个候选框，可以得到2000\*4096这样的特征向量矩阵，然后我们只需要把这样的一个矩阵与svm权值矩阵4096\*N点乘(N为分类类别数目，因为我们训练的N个svm，每个svm包好了4096个W)，就可以得到结果了。

OK，就讲到这边吧，懒得打字了，打到手酸。

个人总结：学习这篇文献最大的意义在于作者把自己的试验过程都讲的很清楚，可以让我们学到不少的调参经验，真的是很佩服作者背后的思考。因为文献很长、细节非常之多，本人也对物体检测不感兴趣，只是随便看看文献、学学算法罢了，所以很多细节没有细看，比如7.3 bounding box的回归过程；最后看这篇文献好累、十几页，细节一大堆，包含作者各种实验、思考……

参考文献：

1、《Rich feature hierarchies for Accurate Object Detection and Segmentation》

2、《Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition》

\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*作者：hjimce 时间：2015.12.3 联系QQ：1393852684 原创文章，转载请保留原文地址、作者等信息\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*

R-CNN problem：

1. Slow at test-time: need to run full forward pass of CNN for each region proposal
2. SVMs and regressors are post-hoc: CNN features not updated in response to SVMs and regressors
3. Complex multistage training pipeline

### 

SPP net （Spatial Pyramid Pooling）

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/F877B582044E8D75D547F06C723175EE.jpg)

R-CNN主要存在的问题是对于提取的每个Region Proposal，都要分别进行CNN前向传播一次（相当于进行了2000吃提特征和SVM分类的过程），计算量较大，而且CNN的模型确定的情况下只能接受固定大小的输入（也即wraped region的大小固定）。既然所有的Region Proposal都在输入图像中，与其提取后分别作为CNN的输入，为什么不考虑将带有Region Proposal的原图像直接作为CNN的输入呢？原图像在经过CNN的卷积层得到feature map，原图像中的Region Proposal经过特征映射（也即CNN的卷积下采样等操作）也与feature map中的一块儿区域相对应。

SPP net中Region Proposal仍然是在原始输入图像中选取的，不过是通过CNN映射到了feature map中的一片区域。空间金字塔池化的思想是：对卷积层的feature map上的Region Proposal映射区域分别划分成1×1，2×2，4×4的窗口（window），并在每个窗口内做max pooling，这样对于一个卷积核产生的feature map，就可以由SPP得到一个（1×1+2×2+4×4）维的特征向量。论文中采用的网络结构最后一层卷积层共有256个卷积核，所以最后会得到一个固定维度的特征向量（1×1+2×2+4×4）×256维），并用此特征向量作为全连接层的输入后做分类。

相比于R-CNN，SPP net是使用原始图像作为CNN网络的输入来计算feature map（R-CNN中是每个Region Proposal都要经历一次CNN计算），这样就大大减少了计算量。另外，SPP net中Region Proposal仍然是通过选择性搜索等算法在输入图像中生成的，通过映射的方式得到feature map中对应的区域，并对Region Proposal在feature map中对应区域做空间金字塔池化。通过空间金字塔池化操作，对于任意尺寸的候选区域，经过SPP后都会得到固定长度的特征向量。

使用SPP net相比于R-CNN可以大大加快目标检测速度，但是依然存在着一些问题（这些问题是从Fast R-CNN的论文中看的，现在还不是很理解）：

1\. 训练分多个阶段，步骤繁琐（微调网络+训练SVM+训练边框回归器）；

2\. SPP net在微调网络的时候固定了卷积层，只对全连接层进行微调（也即微调算法不能调节空间金字塔池化层前面的卷积层）；

FAST R-CNN

转载自http://zhangliliang.com/2015/05/17/paper-note-fast-rcnn/

论文出处见：<http://arxiv.org/abs/1504.08083>
项目见：<https://github.com/rbgirshick/fast-rcnn>

R-CNN的进化版，0.3s一张图片，VOC07有70的mAP，可谓又快又强。
而且rbg的代码一般写得很好看，应该会是个很值得学习的项目。

动机
--

为何有了R-CNN和SPP-Net之后还要提出Fast RCNN（简称FRCN）？因为前者有三个缺点

* 训练的时候，pipeline是隔离的，先提proposal，然后CNN提取特征，之后用SVM分类器，最后再做bbox regression。FRCN实现了end-to-end的joint training(提proposal阶段除外)。
* 训练时间和空间开销大。RCNN中ROI-centric的运算开销大，所以FRCN用了image-centric的训练方式来通过卷积的share特性来降低运算开销；RCNN提取特征给SVM训练时候需要中间要大量的磁盘空间存放特征，FRCN去掉了SVM这一步，所有的特征都暂存在显存中，就不需要额外的磁盘空间了。
* 测试时间开销大。依然是因为ROI-centric的原因，这点SPP-Net已经改进，然后FRCN进一步通过single scale testing和SVD分解全连接来提速。

整体框架
----

整体框架如Figure 1，如果以AlexNet（5个卷积和3个全连接）为例，大致的训练过程可以理解为：

1. selective search在一张图片中得到约2k个object proposal(这里称为RoI)
2. 缩放图片的scale得到图片金字塔，FP得到conv5的特征金字塔。
3. 对于每个scale的每个ROI，求取映射关系，在conv5中crop出对应的patch。并用一个单层的SPP layer（这里称为Rol pooling layer）来统一到一样的尺度（对于AlexNet是6x6）。
4. 继续经过两个全连接得到特征，这特征有分别share到两个新的全连接，连接上两个优化目标。第一个优化目标是分类，使用softmax，第二个优化目标是bbox regression，使用了一个smooth的L1-loss.

除了1，上面的2-4是joint training的。
测试时候，在4之后做一个NMS即可。

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/240BA9A4C8DE141759AAF9C0C34CA782.jpg)
[![]()](http://hexo-pic-zhangliliang.qiniudn.com/%E5%B0%8FQ%E6%88%AA%E5%9B%BE-20150517103126.png)

整体框架大致如上述所示了，对比回来SPP-Net，可以看出FRCN大致就是一个joint training版本的SPP-Net，改进如下：

1. SPP-Net在实现上无法同时tuning在SPP layer两边的卷积层和全连接层。
2. SPP-Net后面的需要将第二层FC的特征放到硬盘上训练SVM，之后再额外训练bbox regressor。

接下来会介绍FRCN里面的一些细节的motivation和效果。

Rol pooling layer
-----------------

Rol pooling layer的作用主要有两个，一个是将image中的rol定位到feature map中对应patch，另一个是用一个单层的SPP layer将这个feature map patch下采样为大小固定的feature再传入全连接层。
这里有几个细节。

1. 对于某个rol，怎么求取对应的feature map patch？这个论文没有提及，笔者也还没有仔细去抠，觉得这个问题可以到代码中寻找。:)
2. 为何只是一层的SPP layer？多层的SPP layer不会更好吗？对于这个问题，笔者认为是因为需要读取pretrain model来finetuning的原因，比如VGG就release了一个19层的model，如果是使用多层的SPP layer就不能够直接使用这个model的parameters，而需要重新训练了。

Multi-task loss
---------------

FRCN有两个loss，以下分别介绍。
对于分类loss，是一个N+1路的softmax输出，其中的N是类别个数，1是背景。为何不用SVM做分类器了？在5.4作者讨论了softmax效果比SVM好，因为它引入了类间竞争。（笔者觉得这个理由略牵强，估计还是实验效果验证了softmax的performance好吧 ^\_^）
对于回归loss，是一个4xN路输出的regressor，也就是说对于每个类别都会训练一个单独的regressor的意思，比较有意思的是，这里regressor的loss不是L2的，而是一个平滑的L1，形式如下：

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/D0719651064D235DF18EA7F65E63041E.jpg)
[![]()](http://hexo-pic-zhangliliang.qiniudn.com/%E5%B0%8FQ%E6%88%AA%E5%9B%BE-20150517114340.png)
作者这样设置的目的是想让loss对于离群点更加鲁棒，控制梯度的量级使得训练时不容易跑飞。
最后在5.1的讨论中，作者说明了Multitask loss是有助于网络的performance的。

Scale invariance
----------------

这里讨论object的scale问题，就是网络对于object的scale应该是要不敏感的。这里还是引用了SPP的方法，有两种。

1. brute force （single scale），也就是简单认为object不需要预先resize到类似的scale再传入网络，直接将image定死为某种scale，直接输入网络来训练就好了，然后期望网络自己能够学习到scale-invariance的表达。
2. image pyramids （multi scale），也就是要生成一个金字塔，然后对于object，在金字塔上找到一个大小比较接近227x227的投影版本，然后用这个版本去训练网络。

可以看出，2应该比1更加好，作者也在5.2讨论了，2的表现确实比1好，但是好的不算太多，大概是1个mAP左右，但是时间要慢不少，所以作者实际采用的是第一个策略，也就是single scale。
这里，FRCN测试之所以比SPP快，很大原因是因为这里，因为SPP用了2，而FRCN用了1。

SVD on fc layers
----------------

对应文中3.1，这段笔者没细看。大致意思是说全连接层耗时很多，如果能够简化全连接层的计算，那么能够提升速度。
具体来说，作者对全连接层的矩阵做了一个SVD分解，mAP几乎不怎么降（0.3%），但速度提速30%

Which layers to finetune?
-------------------------

对应文中4.5，作者的观察有2点

1. 对于较深的网络，比如VGG，卷积层和全连接层是否一起tuning有很大的差别（66.9 vs 61.4）
2. 有没有必要tuning所有的卷积层？答案是没有。如果留着浅层的卷积层不tuning，可以减少训练时间，而且mAP基本没有差别。

Data augment
------------

在训练期间，作者做过的唯一一个数据增量的方式是水平翻转。
作者也试过将VOC12的数据也作为拓展数据加入到finetune的数据中，结果VOC07的mAP从66.9到了70.0，说明对于网络来说，数据越多就是越好的。

Are more proposals always better？
---------------------------------

对应文章的5.5，答案是NO。
作者将proposal的方法粗略地分成了sparse（比如selective search）和dense（sliding windows）。

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/42AB705B8F50494D53562ACDE75AE098.jpg)
如Figure 3所示，不管是哪种方法，盲目增加proposal个数反而会损害到mAP的。
[](http://hexo-pic-zhangliliang.qiniudn.com/%E5%B0%8FQ%E6%88%AA%E5%9B%BE-20150517125419.png)![]()
作者引用了文献11的一句话来说明：““[sparse proposals] may improve detection quality by reducing spurious false positives.”

Faster R-CNN

转自：<http://blog.csdn.net/zy1034092330/article/details/62044941>

Faster RCNN github : https://github.com/rbgirshick/py-faster-rcnn

Faster RCNN paper : https://arxiv.org/abs/1506.01497

Bound box regression详解 : http://download.csdn.net/download/zy1034092330/9940097（[来源：](http://caffecn.cn/?/question/160)[王斌\_ICT](http://caffecn.cn/?/question/160)）

缩进经过RCNN和Fast RCNN的积淀，Ross B. Girshick在2016年提出了新的Faster RCNN，在结构上，Faster RCN已经将特征抽取(feature extraction)，proposal提取，bounding box regression(rect refine)，classification都整合在了一个网络中，使得综合性能有较大提高，在检测速度方面尤为明显。

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/C0172BE282021A1029F7B72B51079FFE.jpg)

*图1 Faster CNN基本结构（来自原论文）*

缩进依作者看来，如图1，Faster RCNN其实可以分为4个主要内容：

1. Conv layers。作为一种CNN网络目标检测方法，Faster RCNN首先使用一组基础的conv+relu+pooling层提取image的feature maps。该feature maps被共享用于后续RPN层和全连接层。
2. Region Proposal Networks。RPN网络用于生成region proposals。该层通过softmax判断anchors属于foreground或者background，再利用bounding box regression修正anchors获得精确的proposals。
3. Roi Pooling。该层收集输入的feature maps和proposals，综合这些信息后提取proposal feature maps，送入后续全连接层判定目标类别。
4. Classification。利用proposal feature maps计算proposal的类别，同时再次bounding box regression获得检测框最终的精确位置。

所以本文以上述4个内容作为切入点介绍Faster RCNN网络。

缩进图2展示了python版本中的VGG16模型中的faster\_rcnn\_test.pt的网络结构，可以清晰的看到该网络对于一副任意大小PxQ的图像，首先缩放至固定大小MxN，然后将MxN图像送入网络；而Conv layers中包含了13个conv层+13个relu层+4个pooling层；RPN网络首先经过3x3卷积，再分别生成foreground anchors与bounding box regression偏移量，然后计算出proposals；而Roi Pooling层则利用proposals从feature maps中提取proposal feature送入后续全连接和softmax网络作classification（即分类proposal到底是什么object）。

path:${py-faster-rcnn-root}/models/pascal\_voc/VGG16/faster\_rcnn\_alt\_opt/faster\_rcnn\_test.pt

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/E64A99B38F411C337F538EB5F093BDF3.png)

*图2 faster\_rcnn\_test.pt网络结构*

1 Conv layers
-------------

缩进Conv layers包含了conv，pooling，relu三种层。以python版本中的VGG16模型中的faster\_rcnn\_test.pt的网络结构为例，如图2，Conv layers部分共有13个conv层，13个relu层，4个pooling层。这里有一个非常容易被忽略但是又无比重要的信息，在Conv layers中：

1. 所有的conv层都是：kernel\_size=3，pad=1
2. 所有的pooling层都是：kernel\_size=2，stride=2

为何重要？在Faster RCNN Conv layers中对所有的卷积都做了扩边处理（pad=1，即填充一圈0），导致原图变为(M+2)x(N+2)大小，再做3x3卷积后输出MxN。正是这种设置，导致Conv layers中的conv层不改变输入和输出矩阵大小。如图3：

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/3C772E9ED555EB86A97EF9C08BF563C9.jpg)

*图3*

类似的是，Conv layers中的pooling层kernel\_size=2，stride=2。这样每个经过pooling层的MxN矩阵，都会变为(M/2)\*(N/2)大小。综上所述，在整个Conv layers中，conv和relu层不改变输入输出大小，只有pooling层使输出长宽都变为输入的1/2。

缩进那么，一个MxN大小的矩阵经过Conv layers固定变为(M/16)x(N/16)！这样Conv layers生成的featuure map中都可以和原图对应起来。

2 Region Proposal Networks(RPN)
-------------------------------

缩进经典的检测方法生成检测框都非常耗时，如OpenCV adaboost使用滑动窗口+图像金字塔生成检测框；或如RCNN使用SS(Selective Search)方法生成检测框。而Faster RCNN则抛弃了传统的滑动窗口和SS方法，直接使用RPN生成检测框，这也是Faster RCNN的巨大优势，能极大提升检测框的生成速度。

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/1908FEEABA591D28BEE3C4A754CCA282.jpg)

*图4 RPN网络结构*

上图4展示了RPN网络的具体结构。可以看到RPN网络实际分为2条线，上面一条通过softmax分类anchors获得foreground和background（检测目标是foreground），下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的proposal。而最后的Proposal层则负责综合foreground anchors和bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。其实整个网络到了Proposal Layer这里，就完成了相当于目标定位的功能。

### 2.1 多通道图像卷积基础知识介绍

缩进在介绍RPN前，还要多解释几句基础知识，已经懂的看官老爷跳过就好。

1. 对于单通道图像+单卷积核做卷积，第一章中的图3已经展示了；
2. 对于多通道图像+多卷积核做卷积，计算方式如下：

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/9469BCFF648132403279C8497B39F201.png)

*图5 多通道+多卷积核做卷积示意图（摘自Theano教程）*

缩进如图5，输入图像layer m-1有4个通道，同时有2个卷积核w1和w2。对于卷积核w1，先在输入图像4个通道分别作卷积，再将4个通道结果加起来得到w1的卷积输出；卷积核w2类似。所以对于某个卷积层，无论输入图像有多少个通道，输出图像通道数总是等于卷积核数量！

缩进对多通道图像做1x1卷积，其实就是将输入图像于每个通道乘以卷积系数后加在一起，即相当于把原图像中本来各个独立的通道“联通”在了一起。

### 2.2 anchors

缩进提到RPN网络，就不能不说anchors。所谓anchors，实际上就是一组由rpn/generate\_anchors.py生成的矩形。直接运行作者demo中的generate\_anchors.py可以得到以下输出：

**[python]** [view plain](http://blog.csdn.net/zy1034092330/article/details/62044941# "view plain") [copy](http://blog.csdn.net/zy1034092330/article/details/62044941# "copy")

1. [[ -84. -40. 99. 55.]
2. [-176. -88. 191. 103.]
3. [-360. -184. 375. 199.]
4. [ -56. -56. 71. 71.]
5. [-120. -120. 135. 135.]
6. [-248. -248. 263. 263.]
7. [ -36. -80. 51. 95.]
8. [ -80. -168. 95. 183.]
9. [-168. -344. 183. 359.]]

其中每行的4个值[x1,y1,x2,y2]代表矩形左上和右下角点坐标。9个矩形共有3种形状，长宽比为大约为：width:height = [1:1, 1:2, 2:1]三种，如图6。实际上通过anchors就引入了检测中常用到的多尺度方法。

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/7ABEAD97EFCC46A3EE5B030A2151643F.jpg)

*图6 anchors示意图*

注：关于上面的anchors size，其实是根据检测图像设置的。在python demo中，会把任意大小的输入图像reshape成800x600（即图2中的M=800，N=600）。再回头来看anchors的大小，anchors中长宽1:2中最大为352x704，长宽2:1中最大736x384，基本是cover了800x600的各个尺度和形状。

那么这9个anchors是做什么的呢？借用Faster RCNN论文中的原图，如图7，遍历Conv layers计算获得的feature maps，为每一个点都配备这9种anchors作为初始的检测框。（意思是对于每一个得到的feature map,将图六的九个框放到feature map中的每个点上，以确定这个点用多大的滑窗可以得到要识别的物体）这样做获得检测框很不准确，不用担心，后面还有2次bounding box regression可以修正检测框位置。

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/C93DB71CC8F4F4FD8CFB4EF2E2CEF4F4.jpg)

*图7*

解释一下上面这张图的数字。

1. 在原文中使用的是ZF model中，其Conv Layers中最后的conv5层num\_output=256，对应生成256张特征图，所以相当于feature map每个点都是256-d
2. 在conv5之后，做了rpn\_conv/3x3卷积且num\_output=256，相当于每个点又融合了周围3x3的空间信息（猜测这样做也许更鲁棒？反正我没测试），同时256-d不变（如图4和图7中的红框）
3. 假设在conv5 feature map中每个点上有k个anchor（默认k=9），而每个anhcor要分foreground和background，所以每个点由256d feature转化为cls=2k scores；而每个anchor都有[x, y, w, h]对应4个偏移量，所以reg=4k coordinates
4. 补充一点，全部anchors拿去训练太多了，训练程序会在合适的anchors中**随机**选取128个postive anchors+128个negative anchors进行训练（什么是合适的anchors下文5.1有解释）

注意，在本文讲解中使用的VGG conv5 num\_output=512，所以是512d，其他类似.....

### 2.3 softmax判定foreground与background

缩进一副MxN大小的矩阵送入Faster RCNN网络后，到RPN网络变为(M/16)x(N/16)，不妨设W=M/16，H=N/16。在进入reshape与softmax之前，先做了1x1卷积，如图8：

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/1AB4B6C3DD607A5035B5203C76B078F3.jpg)

*图8 RPN中判定fg/bg网络结构*

该1x1卷积的caffe prototxt定义如下：

**[cpp]** [view plain](http://blog.csdn.net/zy1034092330/article/details/62044941# "view plain") [copy](http://blog.csdn.net/zy1034092330/article/details/62044941# "copy")

1. layer {
2. name: "rpn\_cls\_score"
3. type: "Convolution"
4. bottom: "rpn/output"
5. top: "rpn\_cls\_score"
6. convolution\_param {
7. num\_output: 18 \# 2(bg/fg) \* 9(anchors)
8. kernel\_size: 1 pad: 0 stride: 1
9. }
10. }

可以看到其num\_output=18，也就是经过该卷积的输出图像为WxHx18大小（注意第二章开头提到的卷积计算方式）。这也就刚好对应了feature maps每一个点都有9个anchors，同时每个anchors又有可能是foreground和background，所有这些信息都保存WxHx(9x2)大小的矩阵。为何这样做？后面接softmax分类获得foreground anchors，也就相当于初步提取了检测目标候选区域box（一般认为目标在foreground anchors中）。

缩进那么为何要在softmax前后都接一个reshape layer？其实只是为了便于softmax分类，至于具体原因这就要从caffe的实现形式说起了。在caffe基本数据结构blob中以如下形式保存数据：

**blob=[batch\_size, channel，height，width]**

对应至上面的保存bg/fg anchors的矩阵，其在caffe blob中的存储形式为[1, 2\*9, H, W]。而在softmax分类时需要进行fg/bg二分类，所以reshape layer会将其变为[1, 2, 9\*H, W]大小，即单独“腾空”出来一个维度以便softmax分类，之后再reshape回复原状。贴一段caffe softmax\_loss\_layer.cpp的reshape函数的解释，非常精辟：

**[cpp]** [view plain](http://blog.csdn.net/zy1034092330/article/details/62044941# "view plain") [copy](http://blog.csdn.net/zy1034092330/article/details/62044941# "copy")

1. "Number of labels must match number of predictions; "
2. "e.g., if softmax axis == 1 and prediction shape is (N, C, H, W), "
3. "label count (number of labels) must be N\*H\*W, "
4. "with integer values in {0, 1, ..., C-1}.";

综上所述，RPN网络中利用anchors和softmax初步提取出foreground anchors作为候选区域。

### 2.4 bounding box regression原理

缩进介绍bounding box regression数学模型及原理。如图9所示绿色框为飞机的Ground Truth(GT)，红色为提取的foreground anchors，那么即便红色的框被分类器识别为飞机，但是由于红色的框定位不准，这张图相当于没有正确的检测出飞机。所以我们希望采用一种方法对红色的框进行微调，使得foreground anchors和GT更加接近。

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/93021A3C03D66456150EFA1DA95416D3.jpg)

*图9*

缩进对于窗口一般使用四维向量(x, y, w, h)表示，分别表示窗口的中心点坐标和宽高。对于图 10，红色的框A代表原始的Foreground Anchors，绿色的框G代表目标的GT，我们的目标是寻找一种关系，使得输入原始的anchor A经过映射得到一个跟真实窗口G更接近的回归窗口G'，即：给定anchor A=(Ax, Ay, Aw, Ah)，GT=[Gx, Gy, Gw, Gh]，寻找一种变换**F**：使得**F**(Ax, Ay, Aw, Ah)=(G'x, G'y, G'w, G'h)，其中(G'x, G'y, G'w, G'h)≈(Gx, Gy, Gw, Gh)。

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/EA7E6E48662BFA68EC73BDF32F36BB85.jpg)

*图10*

那么经过何种变换**F**才能从图6中的anchor A变为G'呢？ 比较简单的思路就是:

缩进 1\. 先做平移

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/A9380736B49A548736B35D182FFD44AB.jpg)

缩进 2\. 再做缩放

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/C4D9C89C3FB1BAA90631F662F906626F.jpg)

缩进观察上面4个公式发现，需要学习的是dx(A)，dy(A)，dw(A)，dh(A)这四个变换。当输入的anchor A与GT相差较小时，可以认为这种变换是一种线性变换， 那么就可以用线性回归来建模对窗口进行微调（注意，只有当anchors A和GT比较接近时，才能使用线性回归模型，否则就是复杂的非线性问题了）。对应于Faster RCNN原文，平移量(tx, ty)与尺度因子(tw, th)如下：

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/AABFC5609141B8C25C10DEA1C8E25040.jpg)

缩进接下来的问题就是如何通过线性回归获得dx(A)，dy(A)，dw(A)，dh(A)了。线性回归就是给定输入的特征向量X, 学习一组参数W, 使得经过线性回归后的值跟真实值Y非常接近，即Y=WX。对于该问题，输入X是一张经过卷积获得的feature map，定义为Φ；同时还有训练传入的GT，即(tx, ty, tw, th)。输出是dx(A)，dy(A)，dw(A)，dh(A)四个变换。那么目标函数可以表示为：

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/0DAD3F869B9C1760C7188EFD0B6F81F1.png)

其中Φ(A)是对应anchor的feature map组成的特征向量，w是需要学习的参数，d(A)是得到的预测值（\*表示 x，y，w，h，也就是每一个变换对应一个上述目标函数）。为了让预测值(tx, ty, tw, th)与真实值差距最小，设计损失函数：

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/C898FC9738B82AFA2729A5A5F61AC893.jpg)

函数优化目标为：

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/1E67089E47548F8A383A221F184DEA04.jpg)

### 2.5 对proposals进行bounding box regression

缩进在了解bounding box regression后，再回头来看RPN网络第二条线路，如图11。

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/8241C8076D60156248916FE2F1A5674A.jpg)

*图11 RPN中的bbox reg*

先来看一看上图11中1x1卷积的caffe prototxt定义：

**[cpp]** [view plain](http://blog.csdn.net/zy1034092330/article/details/62044941# "view plain") [copy](http://blog.csdn.net/zy1034092330/article/details/62044941# "copy")

1. layer {
2. name: "rpn\_bbox\_pred"
3. type: "Convolution"
4. bottom: "rpn/output"
5. top: "rpn\_bbox\_pred"
6. convolution\_param {
7. num\_output: 36 \# 4 \* 9(anchors)
8. kernel\_size: 1 pad: 0 stride: 1
9. }
10. }

可以看到其num\_output=36，即经过该卷积输出图像为WxHx36，在caffe blob存储为[1, 36, H, W]，这里相当于feature maps每个点都有9个anchors，每个anchors又都有4个用于回归的[dx(A)，dy(A)，dw(A)，dh(A)]变换量。

### 2.6 Proposal Layer

缩进Proposal Layer负责综合所有[dx(A)，dy(A)，dw(A)，dh(A)]变换量和foreground anchors，计算出精准的proposal，送入后续RoI Pooling Layer。还是先来看看Proposal Layer的caffe prototxt定义：

**[cpp]** [view plain](http://blog.csdn.net/zy1034092330/article/details/62044941# "view plain") [copy](http://blog.csdn.net/zy1034092330/article/details/62044941# "copy")

1. layer {
2. name: 'proposal'
3. type: 'Python'
4. bottom: 'rpn\_cls\_prob\_reshape'
5. bottom: 'rpn\_bbox\_pred'
6. bottom: 'im\_info'
7. top: 'rois'
8. python\_param {
9. module: 'rpn.proposal\_layer'
10. layer: 'ProposalLayer'
11. param\_str: "'feat\_stride': 16"
12. }
13. }

Proposal Layer有3个输入：fg/bg anchors分类器结果rpn\_cls\_prob\_reshape，对应的bbox reg的[dx(A)，dy(A)，dw(A)，dh(A)]变换量rpn\_bbox\_pred，以及im\_info；另外还有参数feat\_stride=16，这和图4是对应的。

缩进首先解释im\_info。对于一副任意大小PxQ图像，传入Faster RCNN前首先reshape到固定MxN，im\_info=[M, N, scale\_factor]则保存了此次缩放的所有信息。然后经过Conv Layers，经过4次pooling变为WxH=(M/16)x(N/16)大小，其中feature\_stride=16则保存了该信息，用于计算anchor偏移量。

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/1E43500C7CC9A9DE211D737BC347CED9.jpg)

*图12*

缩进Proposal Layer forward（caffe layer的前传函数）按照以下顺序依次处理：

1. 生成anchors，利用[dx(A)，dy(A)，dw(A)，dh(A)]对所有的anchors做bbox regression回归（这里的anchors生成和训练时完全一致）
2. 按照输入的foreground softmax scores由大到小排序anchors，提取前pre\_nms\_topN(e.g. 6000)个anchors，即提取修正位置后的foreground anchors。
3. 利用im\_info将fg anchors从MxN尺度映射回PxQ原图，判断fg anchors是否大范围超过边界，剔除严重超出边界fg anchors。
4. 进行nms（nonmaximum suppression，非极大值抑制）
5. 再次按照nms后的foreground softmax scores由大到小排序fg anchors，提取前post\_nms\_topN(e.g. 300)结果作为proposal输出。

之后输出proposal=[x1, y1, x2, y2]，注意，由于在第三步中将anchors映射回原图判断是否超出边界，所以这里输出的proposal是对应MxN输入图像尺度的，这点在后续网络中有用。另外我认为，严格意义上的检测应该到此就结束了，后续部分应该属于识别了~

RPN网络结构就介绍到这里，总结起来就是：

**生成anchors -\> softmax分类器提取fg anchors -\> bbox reg回归fg anchors -\> Proposal Layer生成proposals**

****

3 RoI pooling

--------------

缩进而RoI Pooling层则负责收集proposal，并计算出proposal feature maps，送入后续网络。从图2中可以看到Rol pooling层有2个输入：

1. 原始的feature maps
2. RPN输出的proposal boxes（大小各不相同）

### 3.1 为何需要RoI Pooling

缩进先来看一个问题：对于传统的CNN（如AlexNet，VGG），当网络训练好后输入的图像尺寸必须是固定值，同时网络输出也是固定大小的vector or matrix。如果输入图像大小不定，这个问题就变得比较麻烦。有2种解决办法：

1. 从图像中crop一部分传入网络
2. 将图像warp成需要的大小后传入网络

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/E525342CBDE476A11C48A6BE393F226C.jpg)

*图13 crop与warp破坏图像原有结构信息*

两种办法的示意图如图13，可以看到无论采取那种办法都不好，要么crop后破坏了图像的完整结构，要么warp破坏了图像原始形状信息。回忆RPN网络生成的proposals的方法：对foreground anchors进行bound box regression，那么这样获得的proposals也是大小形状各不相同，即也存在上述问题。所以Faster RCNN中提出了RoI Pooling解决这个问题（需要说明，RoI Pooling确实是从SPP发展而来，但是限于篇幅这里略去不讲，有兴趣的读者可以自行查阅相关论文）。

### 3.2 RoI Pooling原理

缩进分析之前先来看看RoI Pooling Layer的caffe prototxt的定义：

**[cpp]** [view plain](http://blog.csdn.net/zy1034092330/article/details/62044941# "view plain") [copy](http://blog.csdn.net/zy1034092330/article/details/62044941# "copy")

1. layer {
2. name: "roi\_pool5"
3. type: "ROIPooling"
4. bottom: "conv5\_3"
5. bottom: "rois"
6. top: "pool5"
7. roi\_pooling\_param {
8. pooled\_w: 7
9. pooled\_h: 7
10. spatial\_scale: 0.0625 \# 1/16
11. }
12. }

其中有新参数pooled\_w=pooled\_h=7，另外一个参数spatial\_scale=1/16应该能够猜出大概吧。

缩进RoI Pooling layer forward过程：在之前有明确提到：proposal=[x1, y1, x2, y2]是对应MxN尺度的，所以首先使用spatial\_scale参数将其映射回(M/16)x(N/16)大小的feature maps尺度（这里来回多次映射，是有点绕）；之后将每个proposal水平和竖直都分为7份，对每一份都进行max pooling处理。这样处理后，即使大小不同的proposal，输出结果都是7x7大小，实现了fixed-length output（固定长度输出）。

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/E3108DC5CDD76B871E21A4CB64001B5C.jpg)

**

*图14 proposal示意图*

4 Classification
----------------

缩进Classification部分利用已经获得的proposal feature maps，通过full connect层与softmax计算每个proposal具体属于那个类别（如人，车，电视等），输出cls\_prob概率向量；同时再次利用bounding box regression获得每个proposal的位置偏移量bbox\_pred，用于回归更加精确的目标检测框。Classification部分网络结构如图15。

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/9377A45DC8393D546B7B52A491414DED.jpg)

*图15 Classification部分网络结构图*

从PoI Pooling获取到7x7=49大小的proposal feature maps后，送入后续网络，可以看到做了如下2件事：

1. 通过全连接和softmax对proposals进行分类，这实际上已经是识别的范畴了
2. 再次对proposals进行bounding box regression，获取更高精度的rect box

这里来看看全连接层InnerProduct layers，简单的示意图如图16，

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/38594A97F33FF56FC72542A20A78116D.jpg)

*图16 全连接层示意图*

其计算公式如下：

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/F56D3209F9A7D5F27D77EAD7489AB70F.jpg)

其中W和bias B都是预先训练好的，即大小是固定的，当然输入X和输出Y也就是固定大小。所以，这也就印证了之前Roi Pooling的必要性。到这里，我想其他内容已经很容易理解，不在赘述了。

5 Faster RCNN训练
---------------

缩进Faster CNN的训练，是在已经训练好的model（如VGG\_CNN\_M\_1024，VGG，ZF）的基础上继续进行训练。实际中训练过程分为6个步骤：

1. 在已经训练好的model上，训练RPN网络，对应stage1\_rpn\_train.pt
2. 利用步骤1中训练好的RPN网络，收集proposals，对应rpn\_test.pt
3. 第一次训练Fast RCNN网络，对应stage1\_fast\_rcnn\_train.pt
4. 第二训练RPN网络，对应stage2\_rpn\_train.pt
5. 再次利用步骤4中训练好的RPN网络，收集proposals，对应rpn\_test.pt
6. 第二次训练Fast RCNN网络，对应stage2\_fast\_rcnn\_train.pt

可以看到训练过程类似于一种“迭代”的过程，不过只循环了2次。至于只循环了2次的原因是应为作者提到："A similar alternating training can be run for more iterations, but we have observed negligible improvements"，即循环更多次没有提升了。接下来本章以上述6个步骤讲解训练过程。

### 5.1 训练RPN网络

缩进在该步骤中，首先读取RBG提供的预训练好的model（本文使用VGG），开始迭代训练。来看看stage1\_rpn\_train.pt网络结构，如图17。

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/C39AEF1D06E08E4E0CEC96B10F50A779.jpg)

*图17 stage1\_rpn\_train.pt*

*（考虑图片大小，Conv Layers中所有的层都画在一起了，如红圈所示，后续图都如此处理）*

与检测网络类似的是，依然使用Conv Layers提取feature maps。整个网络使用的Loss如下：

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/1C964D29DADF51BFE82EC783344B899E.jpg)

上述公式中，i表示anchors index，pi表示foreground softmax predict概率，pi\*代表对应的GT predict概率（即当第i个anchor与GT间IoU\>0.7，认为是该anchor是foreground，pi\*=1；反之IoU\<0.3时，认为是该anchor是background，pi\*=0；至于那些0.3\<IoU\<0.7的anchor则不参与训练）；t代表predict bounding box，t\*代表对应foreground anchor对应的GT box。可以看到，整个Loss分为2部分：

1. cls loss，即rpn\_cls\_loss层计算的softmax loss，用于分类anchors为forground与background的网络训练
2. reg loss，即rpn\_loss\_bbox层计算的soomth L1 loss，用于bounding box regression网络训练。注意在该loss中乘了pi\*，相当于只关心foreground anchors的回归（其实在回归中也完全没必要去关心background）。

缩进由于在实际过程中，Ncls和Nreg差距过大，用参数λ平衡二者（如Ncls=256，Nreg=2400时设置λ=10），使总的网络Loss计算过程中能够均匀考虑2种Loss。这里比较重要是Lreg使用的soomth L1 loss，计算公式如下：

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/476FD71F83F92F9638D998C248BD9BE2.jpg)

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/0C13E37C2BB8E1A4F58361EFB39A3795.jpg)

缩进了解数学原理后，反过来看图17：

1. 在RPN训练阶段，rpn-data（python AnchorTargetLayer）层会按照和test阶段Proposal层完全一样的方式生成Anchors用于训练
2. 对于rpn\_loss\_cls，输入的rpn\_cls\_scors\_reshape和rpn\_labels分别对应p与p\*，Ncls参数隐含在p与p\*的caffe blob的大小中
3. 对于rpn\_loss\_bbox，输入的rpn\_bbox\_pred和rpn\_bbox\_targets分别对应t于t\*，rpn\_bbox\_inside\_weigths对应p\*，rpn\_bbox\_outside\_weights对应λ，Nreg同样隐含在caffe blob大小中

这样，公式与代码就完全对应了。特别需要注意的是，在训练和检测阶段生成和存储anchors的顺序完全一样，这样训练结果才能被用于检测！

### 5.2 通过训练好的RPN网络收集proposals

缩进在该步骤中，利用之前的RPN网络，获取proposal rois，同时获取foreground softmax probability，如图18，然后将获取的信息保存在python pickle文件中。该网络本质上和检测中的RPN网络一样，没有什么区别。

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/1AC5F8A2899EE413464ECF7866F8F840.jpg)

*图18 rpn\_test.pt*

### 5.3 训练Faster RCNN网络

缩进读取之前保存的pickle文件，获取proposals与foreground probability。从data层输入网络。然后：

1. 将提取的proposals作为rois传入网络，如图19蓝框
2. 将foreground probability作为bbox\_inside\_weights传入网络，如图19绿框
3. 通过caffe blob大小对比，计算出bbox\_outside\_weights（即λ），如图19绿框

这样就可以训练最后的识别softmax与最终的bounding regression了，如图19。

![](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/FBECE817952865689187E68F0AF86792.jpg)

*图19 stage1\_fast\_rcnn\_train.pt*

之后的训练都是大同小异，不再赘述了。

Faster RCNN还有一种end-to-end的训练方式，可以一次完成train，有兴趣请自己看作者GitHub吧。

PS：我知道你们想问，画图工具：http://ethereon.github.io/netscope/\#/editor

--------------------------------------------------------------------------

Faster RCNN的分析就结束了，之后会缓慢更新YOLO，YOLO V2，SSD，Mask RCNN等内容，敬请期待~

\>\>\>\>\>\>[我的YOLO详解点这里](http://blog.csdn.net/zy1034092330/article/details/72807924)\<\<\<\<\<\<

\>\>\>\>\>\>[我的SSD详解点这里](http://blog.csdn.net/zy1034092330/article/details/72862030)\<\<\<\<\<\<

 ![page110image2800](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/07171BA83628ED0507A726288B645DF5.jpg)

物体检测总结 

**Localization: **

* - Find a fixed number of objects (one or many)
* - L2 regression from CNN features to box coordinates
* - Much simpler than detection; consider it for your projects!
* - Overfeat: Regression + efficient sliding window with FC -\> conv conversion
* - Deeper networks do better

**Object Detection:**

* - Find a variable number of objects by classifying image regions
* - Before CNNs: dense multiscale sliding window
* - Avoid dense sliding window with region proposals
* - R-CNN: Selective Search + CNN classification / regression
* - Fast R-CNN: Swap order of convolutions and region extraction
* - Faster R-CNN: Compute region proposals within the network
* 
* - Deeper networks do better

Part 4\. 其他物体检测算法
=================

R-FCN(2016)

* fully convolutional networks
* Position-sensitive score map
* 分类:translationinvariance
* 检测:translationvariance
* Per-RoI计算特别快

 ![page120image5328](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/EDBEFEC96557328DADE9A3FF6C3CFFF9.png) ![page120image6560](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/CCCA8A00D772595B0A3E8B8DDB5D2827.png) ![page120image6728](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/FF66C34AE3DCF54008E1C6D40D88BF21.jpg) 

J. Dai, R-FCN: Object Detection via Region-based Fully Convolutional Networks, NIPS 2016\. 

 ![page121image952](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/634DE6A50BC1EAE38B5A8CDF76C67074.png) ![page121image1120](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/2B69D60B15D9B50B77E44F8BDCCE5FEA.png) ![page121image1288](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/41AF1E04889B6389925C01A5866E39ED.png) 

YOLO(2016年)

• 无需region proposal(两步验证机制) 

• 把原始图片缩放成448×448大小 

• 运行单个CNN计算:
 – 物体中心是否落入单元格、物体的位置、物体的类别

• You Only Look Once, YOLO

• 每个单元格输出B个矩形框(冗余设计)
 – 包含框的位置信息(x, y, w, h)与物体的Confidence – x,y相对于grid w,h相对于整图 

• 每个单元格再输出C个类别的条件概P(Class∣Object) • 最终输出层应有S×S×(B∗5+C)

• 图片分割成S×S的单元格

 ![page121image4720](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/E6FF6BDF436D0C1C5921AA324EA6B02D.jpg)

 ![page122image5480](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/C75B9D8CF2D6FA7FD7FD90F2B0363E60.png) 

J. Redmon et al, You Only Look Once: Unified, Real-Time Object Detection, CVPR 2016\. 

SSD (2016 年)

* Single Shot Detector
* 在不同层级feature map下进行识别，能够覆盖更多范围
* SSD采用了类似于RPN中anchor box的机制

   ![page124image2840](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/FDAF667C59A8469EDFC8BC46A7CD48D7.png) 

   ![page125image800](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/6EABDC401915BB067CB120A0079EDA29.png) ![page125image968](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/41AF1E04889B6389925C01A5866E39ED.png) 

  Feature Pyramid Network (2016年) 

  Deformable CNN (2017年)

  图像物体检测算法 

  * 传统方法:
 – Adaboost/Haar特征 – 多尺度滑动窗口
  * 深度学习，两步验证策略 (region proposal+verification):

  – RCNN  SPPNet  Fast RCNN  Faster RCNN  R-FCN 

  • 深度学习一步走框架 (direct regression in grids): – YOLO  SSD 

  • 其他改进
 – 金字塔模型(多尺度检测)FPN – 可变形卷积 

  • To be continued **……**

**非极大值抑制**

因为一会儿讲RCNN算法，会从一张图片中找出n多个可能是物体的矩形框，然后为每个矩形框为做类别分类概率：

![](http://img.blog.csdn.net/20160314193846683)

就像上面的图片一样，定位一个车辆，最后算法就找出了一堆的方框，我们需要判别哪些矩形框是没用的。非极大值抑制：先假设有6个矩形框，根据分类器类别分类概率做排序，从小到大分别属于车辆的概率分别为A、B、C、D、E、F。

(1)从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值;

(2)假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。

(3)从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。

就这样一直重复，找到所有被保留下来的矩形框。