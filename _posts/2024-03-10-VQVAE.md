---
layout: post
title:  "VQVAE & VQGAN"
date:   2024-03-10 16:45:01 +0800
categories: deep_learning
tag: stable_diffusion
---


* content
{:toc}
![VQVAE](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/2024-03-10-170201.png)
[VQVAE_苏神](https://kexue.fm/archives/6760)

[VQVAE_others](https://sunlin-ai.github.io/2022/06/02/VQ-VAE.html)

1. 无（自）监督模型。三部分组成：编码器、潜在空间、解码器。输入输出都是真实图像。
2. VQVAE跟VAE（Variational Autoencoders）不一样，它不是一种生成模型，或者说，它自己本身不能通过解码器随机生成图像
跟AE（Autoencoders）作用更相似，是一种图像重建模型。论文里的说法，作用是给图像生成有用的特征表示, 算是一种图像"降维"方法(准确说，维度没变，尺寸变小了)
3. Vector-Quantized的意思就是：特征表示z_q是多个向量组成(矩阵，二维张量)，所谓”离散的“。这跟AE和VAE不一样（它们编码器生成的特征是一个向量，所谓"连续的"）
配合自回归生成模型（比如：PixelCNN），可以生成高质量的图像、视频和语音，并进行高质量的说话人转换和音素的无监督学习
4. VQVAE+PixelCNN如何随机生成图像，分5步：
  - 先拿一个数据集训练VQVAE模型, VQVAE推理模式，将数据集里所有图像输入到VQVAE，编码器输出z_e(x), 计算它们与Code book里每个向量最小距离的索引, 索引位置的向量就是quantize后的结果，z_q
  - 用数据集所有图像编码器特征在Code book中最小距离的索引(组合)，训练PixelCNN模型(这个时候Codebook已经训练好了).
  - PixelCNN推理下，随机一个初始化索引输入，输出的是”有意义“的索引组合
  - PixelCNN生成的索引，再从VQVAE的Code book找到对应的”离散“潜在向量，也就是z_q, 把它扔给VQVAE的解码器，就可以生成”有意义“的图像
5. Quantize的效果： PixelCNN的自回归机制生成图像太慢了？假设一张图片是3通道，224*224，意味着PixelCNN生成一张图片，模型推理的次数是：224*224*3，碰到图像尺寸再大一点，简直是灾难。而且PixelCNN作用在离散空间上->因为它预测的是下一个像素值，在[0, 255]。但是encoder得到的是一个连续的结果，我们需要把连续的结果变成离散的。
5. 就像编码器和解码器网络一样，codebook 通过梯度下降来学习的。理想情况下，我们的编码器将输出一个接近学习到的 codebook 向量。这里本质上存在一个双向问题：学习与编码器输出对齐的 codebook 向量和学习与codebook 向量对齐的编码器输出。

这两个问题可以通过向损失函数添加项来解决。整个VQ-VAE 损失函数是：
$$
log(p(x|q(x)))+||sg[ze(x)]−e||_2^2+β||ze(x)−sg[e]||_2^2
$$

sg[x]代表“停止梯度”。第一项是标准的重构损失；第二项是 codebook 对齐损失，其目标是使所选的 codebook 矢量尽可能接近编码器输出。编码器输出有一个停止梯度运算符，因为这项仅用于更新 codebook。第三项与第二项类似，但它将停止梯度放在 codebook 向量上，因为它旨在更新编码器输出，让其尽可能接近 codebook 向量。这项称为codebook 损失，其对总体损失的重要性由超参数 
β 调整。当然，如果有多个，则最后两项在模型的每个量化向量输出上取平均值。
