---
layout: post
title:  "VQVAE & VQGAN"
date:   2024-03-10 16:45:01 +0800
categories: deep_learning
tag: stable_diffusion
---


* content
{:toc}
### VQVAE
![VQVAE](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/2024-03-10-170201.png)
[VQVAE_苏神](https://kexue.fm/archives/6760)

[VQVAE_others](https://sunlin-ai.github.io/2022/06/02/VQ-VAE.html)

1. 无（自）监督模型。三部分组成：编码器、潜在空间、解码器。输入输出都是真实图像。
2. VQVAE跟VAE（Variational Autoencoders）不一样，它不是一种生成模型，或者说，它自己本身不能通过解码器随机生成图像
跟AE（Autoencoders）作用更相似，是一种图像重建模型。论文里的说法，作用是给图像生成有用的特征表示, 算是一种图像"降维"方法(准确说，维度没变，尺寸变小了)
3. Vector-Quantized的意思就是：特征表示z_q是多个向量组成(矩阵，二维张量)，所谓”离散的“。这跟AE和VAE不一样（它们编码器生成的特征是一个向量，所谓"连续的"）
配合自回归生成模型（比如：PixelCNN），可以生成高质量的图像、视频和语音，并进行高质量的说话人转换和音素的无监督学习
4. VQVAE+PixelCNN如何随机生成图像，分5步：
  - 先拿一个数据集训练VQVAE模型, VQVAE推理模式，将数据集里所有图像输入到VQVAE，编码器输出z_e(x), 计算它们与Code book里每个向量最小距离的索引, 索引位置的向量就是quantize后的结果，z_q
  - 用数据集所有图像编码器特征在Code book中最小距离的索引(组合)，训练PixelCNN模型(这个时候Codebook已经训练好了).
  - PixelCNN推理下，随机一个初始化索引输入，输出的是”有意义“的索引组合
  - PixelCNN生成的索引，再从VQVAE的Code book找到对应的”离散“潜在向量，也就是z_q, 把它扔给VQVAE的解码器，就可以生成”有意义“的图像
5. Quantize的效果： PixelCNN的自回归机制生成图像太慢了。假设一张图片是3通道，224*224，意味着PixelCNN生成一张图片，模型推理的次数是：224*224*3，碰到图像尺寸再大一点，简直是灾难。而且PixelCNN作用在离散空间上->因为它预测的是下一个像素值，在[0, 255]，并且压缩图像的相邻像素块之间差别会比较大，不像原图那样相邻的比较接近。但是encoder得到的是一个连续的结果，我们需要把连续的结果变成离散的。
5. 就像编码器和解码器网络一样，codebook 通过梯度下降来学习的。理想情况下，我们的编码器将输出一个接近学习到的 codebook 向量。这里本质上存在一个双向问题：学习与编码器输出对齐的 codebook 向量和学习与codebook 向量对齐的编码器输出。

这两个问题可以通过向损失函数添加项来解决。整个VQ-VAE 损失函数是：
$$
∥x−decoder(z+sg[zq−z])∥_2^2+β∥sg[z]−zq∥_2^2+γ∥z−sg[zq]∥_2^2
$$


sg[x]代表“停止梯度”。第一项是标准的重构损失；第二项是 codebook 对齐损失，其目标是使所选的 codebook 矢量尽可能接近编码器输出。编码器输出有一个停止梯度运算符，因为这项仅用于更新 codebook。第三项与第二项类似，但它将停止梯度放在 codebook 向量上，因为它旨在更新编码器输出，让其尽可能接近 codebook 向量。这项称为codebook 损失，其对总体损失的重要性由超参数 
β 调整。当然，如果有多个，则最后两项在模型的每个量化向量输出上取平均值。

### VQGAN
![VQGAN](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/2024-03-11-110918.png)
具体来说，VQGAN有三项改进。
1. 使用GPT-2（decoder only）作为codebook index的预测模型，同样是auto-regressive的方法。
2. 作者用感知误差(perceptual loss)代替原来的均方误差作为VQGAN的重建误差。
3. 作者引入了GAN的对抗训练机制，加入了一个基于图块的判别器，把GAN误差加入了总误差。

计算感知误差的方法如下：把两幅图像分别输入VGG，取出中间某几层卷积层的特征，计算特征图像之间的均方误差。如果你之前没学过相关知识，请搜索"perceptual loss"。
基于图块的判别器，即判别器不为整幅图输出一个真或假的判断结果，而是把图像拆成若干图块，分别输出每个图块的判断结果，再对所有图块的判断结果取一个均值。这只是GAN的一种改进策略而已，没有对GAN本身做太大的改动。如果你之前没学过相关知识，请搜索"PatchGAN"。

这样，总的误差可以写成
$$
L = L_{VQ} + \lambda L_{GAN}
$$
其中，$\lambda$是控制两种误差比例的权重。作者在论文中使用了一个公式来自适应地设置$\lambda$。和普通的GAN一样，VQGAN的编码器、解码器（即生成器）、codebook会最小化误差，判别器会最大化误差。

#### 带约束的图像生产
1. 可以把文字embeding对应的index给到decoder only的模型中，引导生产后续的index
2. 如果不是文字的约束，可以考虑对图像模板生成一个vqgan的不同的codebook2，然后以codebook2的index做为输入，来引导生产后续的vector index
TODO: read paper