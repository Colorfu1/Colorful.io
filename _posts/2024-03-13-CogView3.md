---
layout: post
title:  "CogView3: Finer and Faster Text-to-Image Generation via Relay Diffusion"
date:   2024-03-13 13:45:01 +0800
categories: deep_learning
tag: stable_diffusion
---


* content
{:toc}
### CogView3

### Contribution
1. CogView3 executes the task by first creating low-resolution images and subsequently applying relay-based super-resolution(2048 × 2048)
2. CogVIew3 gets competitive text-to-image outputs, greatly reduces both training and inference costs.
3. CogView3 outperforms SDXL, by 77.0% in human evaluations, all while requiring only about 1/2 of the inference time, utilizing 1/10 of the inference time by SDXL after distilled.

### Method
1. we extract triplets of <image, old_cap, new_cap> by automatically prompting GPT-4V. (用GPT-4v 做免费劳动力，还自问自答...), have collected approximately 70,000 recaption triplets with this paradigm and finetune CogVLM-17B by these examples to obtain a **recaption model**, the model is utilized to re-caption the whole training dataset.

![caption collection of GPT-4V](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/2024-03-13-173644.png)

2. training with comprehensive re-cation but inference with brief prompts introduce a missalignment., so we prompt language models to expand user prompts into comprehensive descriptions. expanded prompts to achieve higher preference.

3. The backbone of CogView3 is a 3-billion parameter textto-image diffusion model with a 3-stage UNet architecture, 8× compressed from the pixel space; T5-XXL encoder as the text encoder. user prompts are first rewritten by **recaption model**.

4. 2-stage relay diffusion
  - diffusion model that generates images at a resolution of 512 × 512
  - second stage model performs 2× super-resolution, generating 1024 × 1024 images from 512 × 512 inputs(upsampled).

![Pipeline](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/2024-03-13-180556.png)

5. use Laion-2B as our basic source of the training dataset. replace 95% of the original data captions with the newly-produced captions

6. **Latent Relay Diffusion** 
  - Relay Diffusion的研究动机是从高斯噪声与图像分辨率的关系出发的。实验过程中作者们观察出一个结论：对于更高分辨率的图像进行加噪，其加噪后的结果在低频处有更高的信噪比（Signal-to-Noise Ratio，SNR）
  - 即对于高分辨率的图像，加入同种噪声，其污染程度相较于低分辨率的更轻。对于同一张图像，分辨率越高的图像受到同种高斯噪声的污染程度越轻，对于1024×1024分辨率的图像例子，其污染几乎已经很难用肉眼分辨，这也正是因为低频信息SNR较高导致的
![image show]()
  - Relay Diffusion的解决方案也很直观，称为block noise。简言之，就是将低分辨率的高斯噪声通过一个s×s的卷积核将分辨率映射为原来的s倍，从而适配高分辨率图像的训练。具体来说，如果我们要训练256×256分辨率下的diffusion models，那么s=4，即将64×64的低分辨率噪声映射成256×256的.
  - Relay Diffusion采用的是经典的“生成 + SR”范式完成“接力”，即先生成低分辨率的图像，再将图像通过SR（Super Resolution）上采样为高分辨率图像; 假设要通过64×64的噪声生成一张256×256的图像，Relay Diffusion通过4×4图像块内的“blurring + block noise”的组合来对图像进行降质/恢复处理
  - we implement relay diffusion in the latent space and utilize a simple linear transformation instead of the original patch-wise blurring.
  - Given an image $x_0$ and its low-resolution version $x_L = Downsample(x_0)$, they are first transformed into latent space by the autoencoder as $z_0 = E(x_0), z_L = E(x_L)$. Then the linear blurring transformation is defined as: $$ z_0^t = F(z_0, t) = \frac{T_r - t}{T_r}z_0 + \frac{t}{T_r}z^L, $$
    - $ T_r $ means the starting time for relaying super-resolution 
    - $ z_0^{T_r} $ matches exactly with $z^L$ (t = $T_r$, $z_0^t$ is z^L)
    - the $x_L$ is downsampled then upsampled to the ori resolution
    - 注意到$z_0$其实不是一只不变的，t时刻的$z_0$是插值得到的$z_0^t$
  - forward process $$ q(z_t|z_0) = N(z_t|z_0^t, \sigma_t^2\mathbf{I}), \quad t \in \{1, \ldots, T_r\} $$
  - backward process $$ \mathbb{E}{x_0 \sim p_{\text{data}}} \mathbb{E}_{\epsilon \sim \mathcal{N}(0,\mathbf{I}), t \in \{0,\ldots,T_r\}} \left|| D(z_0^t + \sigma_t \epsilon, t, c_{\text{text}}) - z_0 \right||^2 $$
  - 总结一下，我们有一个原图$x_0$和downsample之后得到的图$x_L$, 这俩图分辨率不一样，然后经过autoencoder将其编码到同样的latent space；用这俩latent space进行插值，得到$z_0^t$, 然后通过给$z_0^t$加噪声的方式获取difussion之后的结果$z_t$; 然后优化的目标是将z_t送到U-Net的denoise模块之后，得到的编码空间的结果和$z_0$的L2 loss.
  -  The equation: $ g(z_t|z_0) = N(z_t|z_0^t, σ_t^2I), t ∈ {1, ..., T_r} $ indicates that $z_t$ is sampled from a Gaussian distribution with mean $z_0^t$ and variance $σ_t^2I$. So $z_t$ is essentially equal to $z_0^t + σ_t * ε$, where ε is a random noise vector sampled from N(0, I), the standard Gaussian distribution. This addition of noise is part of the forward diffusion process.Therefore, $z_0^t + σ_t * ε$ represents the noisy latent representation $z_t$ at time step t in the forward diffusion process.
7. sampler formulation. $X^L$ is generated by base stage and low resolution, upsample to $x^L$, and generate $z_{T_r} = z_0^{T_r} + \sigma_{T_r} \epsilon$ (where $\epsilon$ denotes a unit isotropic Gaussian noise and $z^(T_r)_0 = E(x^L)$ is the latent representation)
![backward process](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/2024-03-14-135308.png)

8. the whole process predict $z_0$ for every $t \in \{T_r, ..., 1\}$, and udate to find the $z_{t-1}$ for next process. 
![Procedure](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/2024-03-14-135910.png)

9. Distillation of relay Diffusion
  - the base stage of CogView3 performs standard diffusion, for the super-resolution stage, we merge the blurring schedule into the diffusion distillation training
  - matching two steps from the latent relaying sampler of the teacher model with one step of the student model
  - teacher steps: $$ z_{t-1} = a_t z_t + b_t \tilde{z}_0(z_t, t)_{teacher} + c_t z_0^t \\
  z_{t-2} = a_{t-1} z_{t-1} + b_{t-1} \tilde{z}_0(z{t-1}, t-1)_{teacher} + c_{t-1} z_0^{t-1}$$
  - student steps:$$\hat{z}_{t-2} = \frac{σ_{t-2}}{σ_t} z_t + \frac{\tilde{z}_0(z_t, t)_{student}}{t} + (\frac{t-2}{t} - \frac{σ_{t-2}}{σ_t}) z_0^t$$
  - incorporate the property of the classifierfree guidance (CFG) strength w, adding learnable projection embeddings of w into timestep embeddings. we implement the incorporation at the first round of the distillation and directly condition onw at subsequent rounds
  - we are able to distribute final sampling steps for relaying distillation as 8 steps for the base stage and 2 steps for the super-resolution stage, or even reduce to 4 steps and 1 step respectively, which achieves both greatly-reduced inference costs and mostly-retained generation quality

### Results
![Table 1](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/2024-03-14-144555.png)
![Table 2](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/2024-03-14-144622.png)
![Table 3](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/2024-03-14-144721.png)
![Fig 4](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/2024-03-14-144733.png)
![Fig 5](https://github.com/Colorfu1/Colorful.io/raw/master/_posts/resources/2024-03-14-144814.png)

  