---
layout: post
title:  "High-Resolution Image Synthesis with Latent Diffusion Models"
date:   2024-03-10 13:45:01 +0800
categories: deep_learning
tag: stable_diffusion
---


* content
{:toc}
CogView3

### Contribution
1. CogView3 executes the task by first creating low-resolution images and subsequently applying relay-based super-resolution(2048 × 2048)
2. CogVIew3 gets competitive text-to-image outputs, greatly reduces both training and inference costs.
3. CogView3 outperforms SDXL, by 77.0% in human evaluations, all while requiring only about 1/2 of the inference time, utilizing 1/10 of the inference time by SDXL after distilled.

### Method
1. we extract triplets CogView3: Finer and Faster Text-to-Image Generation via Relay Diffusion 5of <image, old_cap, new_cap> by automatically prompting GPT-4V. (用GPT-4v 做免费劳动力，还自问自答...), have collected approximately 70,000 recaption triplets with this paradigm and finetune CogVLM-17B by these examples to obtain a **recaption model**, the model is utilized to re-caption the whole training dataset.

![caption collection of GPT-4V]()

2. training with comprehensive re-cation but inference with brief prompts introduce a missalignment., so we prompt language models to expand user prompts into comprehensive descriptions. expanded prompts to achieve higher preference.

3. The backbone of CogView3 is a 3-billion parameter textto-image diffusion model with a 3-stage UNet architecture, 8× compressed from the pixel space; T5-XXL encoder as the text encoder. user prompts are first rewritten by **recaption model**.

4. 2-stage relay diffusion
  - diffusion model that generates images at a resolution of 512 × 512
  - second stage model performs 2× super-resolution, generating 1024 × 1024 images from 512 × 512 inputs(upsampled).

![Pipeline]()

5. use Laion-2B as our basic source of the training dataset. replace 95% of the original data captions with the newly-produced captions

6. **Latent Relay Diffusion** 
  - we implement relay diffusion in the latent space and utilize a simple linear transformation instead of the original patch-wise blurring.
  - Given an image $x_0$ and its low-resolution version $x_L = Downsample(x_0)$, they are first transformed into latent space by the autoencoder as $z_0 = E(x_0), z_L = E(x_L)$. Then the linear blurring transformation is defined as: $$ z_0^t = F(z_0, t) = \frac{T_r - t}{T_r}z_0 + \frac{t}{T_r}z^L, $$
    - $ T_r $ means the starting time for relaying super-resolution 
    - $ z_0^{T_r} $ matches exactly with $z^L$ (t = $T_r$, $z_0^t$ is z^L)
    - the $x_L$ is downsampled then upsampled to the ori resolution
    - 注意到$z_0$其实不是一只不变的，t时刻的$z_0$是插值得到的$z_0^t$
  - forward process $$ q(z_t|z_0) = N(z_t|z_0^t, \sigma_t^2\mathbf{I}), \quad t \in \{1, \ldots, T_r\} $$
  - backward process $$ \mathbb{E}{x_0 \sim p_{\text{data}}} \mathbb{E}_{\epsilon \sim \mathcal{N}(0,\mathbf{I}), t \in \{0,\ldots,T_r\}} \left|| D(z_0^t + \sigma_t \epsilon, t, c_{\text{text}}) - z_0 \right||^2 $$
7. sampler formulation. $X^L$ is generated by base stage and low resolution, upsample to $x^L$, and generate $z_{T_r} = z_0^{T_r} + \sigma_{T_r} \epsilon$ (where $\epsilon$ denotes a unit isotropic Gaussian noise and $z^(T_r)_0 = E(x^L)$ is the latent representation)
![backward process]()

8. the whole process predict $z_0$ for every $t \in \{T_r, ..., 1\}$, and udate to find the $z_{t-1}$ for next process. 
![Procedure]()

9. Distillation of relay Diffusion
  - the base stage of CogView3 performs standard diffusion, for the super-resolution stage, we merge the blurring schedule into the diffusion distillation training
  - matching two steps from the latent relaying sampler of the teacher model with one step of the student model
  - teacher steps: $$ z_{t-1} = a_t z_t + b_t \tilde{z}_0(z_t, t)_{teacher} + c_t z_0^t \\
  z_{t-2} = a_{t-1} z_{t-1} + b_{t-1} \tilde{z}_0(z{t-1}, t-1)_{teacher} + c{t-1} z_0^{t-1}$$
  - student steps:$$\hat{z}_{t-2} = \frac{σ_{t-2}}{σ_t} z_t + \frac{\tilde{z}_0(z_t, t)_{student}}{t} + (\frac{t-2}{t} - \frac{σ_{t-2}}{σ_t}) z_0^t$$
  - incorporate the property of the classifierfree guidance (CFG) strength w, adding learnable projection embeddings of w into timestep embeddings. we implement the incorporation at the first round of the distillation and directly condition onw at subsequent rounds
  - we are able to distribute final sampling steps for relaying distillation as 8 steps for the base stage and 2 steps for the super-resolution stage, or even reduce to 4 steps and 1 step respectively, which achieves both greatly-reduced inference costs and mostly-retained generation quality

  